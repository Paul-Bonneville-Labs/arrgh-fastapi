{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newsletter Processing Pipeline Development\n",
    "\n",
    "This notebook contains the development and testing of the Arrgh! Aggregated Research Repository newsletter processing pipeline.\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "1. **Configuration Setup**: Load environment variables and settings\n",
    "2. **HTML Processing**: Parse and clean newsletter HTML content\n",
    "3. **Entity Extraction**: Use LLM to extract entities (6 types)\n",
    "4. **Graph Operations**: Connect to Neo4j and manage graph database\n",
    "5. **Entity Resolution**: Match extracted entities to existing nodes\n",
    "6. **Fact Extraction**: Extract relationships and temporal facts\n",
    "7. **Graph Updates**: Update Neo4j with new nodes and relationships\n",
    "8. **Summary Generation**: Create human-readable summaries\n",
    "\n",
    "## Entity Types\n",
    "- **Organization**: Companies, institutions, government bodies\n",
    "- **Person**: Individuals mentioned in content\n",
    "- **Product**: Software, hardware, services\n",
    "- **Event**: Conferences, announcements, launches\n",
    "- **Location**: Geographic locations\n",
    "- **Topic**: Subject areas, technologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, TypedDict\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "\n",
    "# Add src directory to path for imports\n",
    "src_path = Path(\"..\") / \"src\"\n",
    "sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(Path(\"..\") / \".env\")\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "\n",
    "# LLM and AI\n",
    "from openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Graph database\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Configuration loaded:\n",
      "  LLM Model: gpt-4-turbo\n",
      "  Neo4j URI: bolt://localhost:7687\n",
      "  Max Entities: 100\n",
      "  Debug Mode: False\n"
     ]
    }
   ],
   "source": [
    "# Configuration from environment variables\n",
    "class Config:\n",
    "    # LLM Configuration\n",
    "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "    LLM_MODEL = os.getenv(\"LLM_MODEL\", \"gpt-4-turbo\")\n",
    "    LLM_TEMPERATURE = float(os.getenv(\"LLM_TEMPERATURE\", \"0.1\"))\n",
    "    LLM_MAX_TOKENS = int(os.getenv(\"LLM_MAX_TOKENS\", \"2000\"))\n",
    "    \n",
    "    # Neo4j Configuration\n",
    "    NEO4J_URI = os.getenv(\"NEO4J_URI\", \"bolt://localhost:7687\")\n",
    "    NEO4J_USER = os.getenv(\"NEO4J_USER\", \"neo4j\")\n",
    "    NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "    NEO4J_DATABASE = os.getenv(\"NEO4J_DATABASE\", \"neo4j\")\n",
    "    \n",
    "    # Processing Configuration\n",
    "    MAX_ENTITIES_PER_NEWSLETTER = int(os.getenv(\"MAX_ENTITIES_PER_NEWSLETTER\", \"100\"))\n",
    "    FACT_EXTRACTION_BATCH_SIZE = int(os.getenv(\"FACT_EXTRACTION_BATCH_SIZE\", \"10\"))\n",
    "    PROCESSING_TIMEOUT = int(os.getenv(\"PROCESSING_TIMEOUT\", \"300\"))\n",
    "    ENTITY_CONFIDENCE_THRESHOLD = float(os.getenv(\"ENTITY_CONFIDENCE_THRESHOLD\", \"0.7\"))\n",
    "    FACT_CONFIDENCE_THRESHOLD = float(os.getenv(\"FACT_CONFIDENCE_THRESHOLD\", \"0.8\"))\n",
    "    \n",
    "    # Feature Flags\n",
    "    ENABLE_DEBUG_MODE = os.getenv(\"ENABLE_DEBUG_MODE\", \"false\").lower() == \"true\"\n",
    "    \n",
    "config = Config()\n",
    "\n",
    "print(\"üîß Configuration loaded:\")\n",
    "print(f\"  LLM Model: {config.LLM_MODEL}\")\n",
    "print(f\"  Neo4j URI: {config.NEO4J_URI}\")\n",
    "print(f\"  Max Entities: {config.MAX_ENTITIES_PER_NEWSLETTER}\")\n",
    "print(f\"  Debug Mode: {config.ENABLE_DEBUG_MODE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Models and Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Data models defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Data models for the processing pipeline\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "class Entity(BaseModel):\n",
    "    \"\"\"Represents an extracted entity from newsletter content.\"\"\"\n",
    "    name: str\n",
    "    type: str  # Organization, Person, Product, Event, Location, Topic\n",
    "    aliases: List[str] = Field(default_factory=list)\n",
    "    confidence: float = Field(ge=0.0, le=1.0)\n",
    "    context: Optional[str] = None\n",
    "    properties: Dict[str, Any] = Field(default_factory=dict)\n",
    "\n",
    "class Fact(BaseModel):\n",
    "    \"\"\"Represents a relationship or fact between entities.\"\"\"\n",
    "    subject_entity: str\n",
    "    predicate: str  # relationship type\n",
    "    object_entity: str\n",
    "    confidence: float = Field(ge=0.0, le=1.0)\n",
    "    temporal_context: Optional[str] = None\n",
    "    date_mentioned: Optional[datetime] = None\n",
    "    source_context: Optional[str] = None\n",
    "\n",
    "class Newsletter(BaseModel):\n",
    "    \"\"\"Represents a newsletter to be processed.\"\"\"\n",
    "    html_content: str\n",
    "    subject: str\n",
    "    sender: str\n",
    "    received_date: Optional[datetime] = None\n",
    "    newsletter_id: Optional[str] = None\n",
    "\n",
    "class ExtractionState(TypedDict):\n",
    "    \"\"\"State for the LangGraph workflow.\"\"\"\n",
    "    # Input\n",
    "    newsletter: Newsletter\n",
    "    \n",
    "    # Processing stages\n",
    "    cleaned_text: str\n",
    "    extracted_entities: List[Entity]\n",
    "    resolved_entities: List[Entity]\n",
    "    extracted_facts: List[Fact]\n",
    "    \n",
    "    # Results\n",
    "    neo4j_updates: Dict[str, Any]\n",
    "    processing_metrics: Dict[str, Any]\n",
    "    text_summary: str\n",
    "    errors: List[str]\n",
    "    \n",
    "    # Metadata\n",
    "    processing_start_time: datetime\n",
    "    current_step: str\n",
    "\n",
    "print(\"üìä Data models defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sample Newsletter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìß Sample newsletter created:\n",
      "  Subject: AI Weekly Newsletter #245\n",
      "  Sender: ai-weekly@example.com\n",
      "  Content length: 1774 characters\n"
     ]
    }
   ],
   "source": [
    "# Sample newsletter HTML for testing\n",
    "sample_newsletter_html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>AI Weekly Newsletter #245</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>AI Weekly Newsletter #245</h1>\n",
    "    <p>Welcome to this week's AI updates!</p>\n",
    "    \n",
    "    <h2>üöÄ Major Announcements</h2>\n",
    "    <p><strong>OpenAI</strong> announced the release of <strong>GPT-5</strong> at their developer conference in <strong>San Francisco</strong>. \n",
    "    CEO <strong>Sam Altman</strong> presented the new capabilities during the <strong>OpenAI DevDay 2024</strong> event.</p>\n",
    "    \n",
    "    <h2>üè¢ Company Updates</h2>\n",
    "    <p><strong>Microsoft</strong> expanded their <strong>Azure AI</strong> services with new enterprise features. \n",
    "    The announcement was made by <strong>Satya Nadella</strong> during the <strong>Microsoft Build 2024</strong> conference.</p>\n",
    "    \n",
    "    <h2>üéØ Industry News</h2>\n",
    "    <p><strong>Google</strong> launched their new <strong>Gemini Pro</strong> model, focusing on <strong>AI Safety</strong> and \n",
    "    <strong>Responsible AI</strong> development. The launch event was held at <strong>Google I/O 2024</strong> in <strong>Mountain View</strong>.</p>\n",
    "    \n",
    "    <h2>üéì Educational Content</h2>\n",
    "    <p><strong>Stanford University</strong> announced a new <strong>AI Safety</strong> course taught by renowned professor \n",
    "    <strong>Fei-Fei Li</strong>. The course will cover <strong>Machine Learning</strong> ethics and <strong>AI Alignment</strong>.</p>\n",
    "    \n",
    "    <h2>üí° Research Highlights</h\n",
    "    <p>New research on <strong>Quantum Computing</strong> applications in <strong>AI</strong> was published by researchers at \n",
    "    <strong>MIT</strong> and <strong>IBM Research</strong>. The paper explores <strong>Quantum Machine Learning</strong> algorithms.</p>\n",
    "    \n",
    "    <p>Thanks for reading! See you next week.</p>\n",
    "    <p>Best regards,<br>The AI Weekly Team</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Create sample newsletter object\n",
    "sample_newsletter = Newsletter(\n",
    "    html_content=sample_newsletter_html,\n",
    "    subject=\"AI Weekly Newsletter #245\",\n",
    "    sender=\"ai-weekly@example.com\",\n",
    "    received_date=datetime.now(timezone.utc),\n",
    "    newsletter_id=\"ai-weekly-245\"\n",
    ")\n",
    "\n",
    "print(\"üìß Sample newsletter created:\")\n",
    "print(f\"  Subject: {sample_newsletter.subject}\")\n",
    "print(f\"  Sender: {sample_newsletter.sender}\")\n",
    "print(f\"  Content length: {len(sample_newsletter.html_content)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. HTML Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ HTML Processing Results:\n",
      "  Cleaned text length: 1159 characters\n",
      "  Title: AI Weekly Newsletter #245\n",
      "  Headers found: 6\n",
      "  Paragraphs found: 7\n",
      "  Links found: 0\n",
      "\n",
      "First 200 characters of cleaned text:\n",
      "'# AI Weekly Newsletter #245 Welcome to this week's AI updates! ## üöÄ Major Announcements **OpenAI** announced the release of **GPT-5** at their developer conference in **San Francisco**. CEO **Sam Altm...'\n"
     ]
    }
   ],
   "source": [
    "def clean_html_content(html_content: str) -> str:\n",
    "    \"\"\"Clean and extract text from HTML content.\"\"\"\n",
    "    try:\n",
    "        # Parse HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Convert to text using html2text for better formatting\n",
    "        h = html2text.HTML2Text()\n",
    "        h.ignore_links = False\n",
    "        h.ignore_images = True\n",
    "        h.body_width = 0  # Don't wrap lines\n",
    "        \n",
    "        # Get text content\n",
    "        text_content = h.handle(str(soup))\n",
    "        \n",
    "        # Clean up extra whitespace\n",
    "        cleaned_text = ' '.join(text_content.split())\n",
    "        \n",
    "        return cleaned_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cleaning HTML: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_sections(html_content: str) -> Dict[str, str]:\n",
    "    \"\"\"Extract different sections of the newsletter.\"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        sections = {\n",
    "            'title': '',\n",
    "            'headers': [],\n",
    "            'paragraphs': [],\n",
    "            'links': []\n",
    "        }\n",
    "        \n",
    "        # Extract title\n",
    "        title_tag = soup.find('title') or soup.find('h1')\n",
    "        if title_tag:\n",
    "            sections['title'] = title_tag.get_text().strip()\n",
    "        \n",
    "        # Extract headers\n",
    "        for header in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "            sections['headers'].append(header.get_text().strip())\n",
    "        \n",
    "        # Extract paragraphs\n",
    "        for para in soup.find_all('p'):\n",
    "            text = para.get_text().strip()\n",
    "            if text:\n",
    "                sections['paragraphs'].append(text)\n",
    "        \n",
    "        # Extract links\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            sections['links'].append({\n",
    "                'text': link.get_text().strip(),\n",
    "                'url': link['href']\n",
    "            })\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting sections: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Test HTML processing\n",
    "cleaned_text = clean_html_content(sample_newsletter.html_content)\n",
    "sections = extract_text_sections(sample_newsletter.html_content)\n",
    "\n",
    "print(\"üßπ HTML Processing Results:\")\n",
    "print(f\"  Cleaned text length: {len(cleaned_text)} characters\")\n",
    "print(f\"  Title: {sections.get('title', 'N/A')}\")\n",
    "print(f\"  Headers found: {len(sections.get('headers', []))}\")\n",
    "print(f\"  Paragraphs found: {len(sections.get('paragraphs', []))}\")\n",
    "print(f\"  Links found: {len(sections.get('links', []))}\")\n",
    "\n",
    "# Show first 200 characters of cleaned text\n",
    "print(f\"\\nFirst 200 characters of cleaned text:\")\n",
    "print(f\"'{cleaned_text[:200]}...'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LLM Integration for Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI client initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI client\n",
    "if config.OPENAI_API_KEY:\n",
    "    openai_client = OpenAI(api_key=config.OPENAI_API_KEY)\n",
    "    llm = ChatOpenAI(\n",
    "        model=config.LLM_MODEL,\n",
    "        temperature=config.LLM_TEMPERATURE,\n",
    "        max_tokens=config.LLM_MAX_TOKENS,\n",
    "        openai_api_key=config.OPENAI_API_KEY\n",
    "    )\n",
    "    print(\"‚úÖ OpenAI client initialized\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è OpenAI API key not found. Please set OPENAI_API_KEY in your .env file\")\n",
    "    llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing entity extraction...\n",
      "‚ùå Error extracting entities: '\\n  \"entities\"'\n",
      "‚ö†Ô∏è No entities extracted\n"
     ]
    }
   ],
   "source": [
    "# Entity extraction prompt template\n",
    "ENTITY_EXTRACTION_PROMPT = \"\"\"\n",
    "You are an expert at extracting structured information from newsletter content. \n",
    "Extract entities from the following newsletter text and classify them into these categories:\n",
    "\n",
    "**Entity Types:**\n",
    "- **Organization**: Companies, institutions, government bodies\n",
    "- **Person**: Individuals mentioned in content\n",
    "- **Product**: Software, hardware, services, models\n",
    "- **Event**: Conferences, announcements, launches\n",
    "- **Location**: Geographic locations (cities, countries, regions)\n",
    "- **Topic**: Subject areas, technologies, fields of study\n",
    "\n",
    "**Instructions:**\n",
    "1. Extract entities with high confidence (>0.7)\n",
    "2. Provide alternative names/aliases if mentioned\n",
    "3. Include context where the entity was mentioned\n",
    "4. Rate confidence from 0.0 to 1.0\n",
    "5. Return results as valid JSON\n",
    "\n",
    "**Newsletter Content:**\n",
    "{content}\n",
    "\n",
    "**Required JSON Format:**\n",
    "```json\n",
    "{\n",
    "  \"entities\": [\n",
    "    {\n",
    "      \"name\": \"Entity Name\",\n",
    "      \"type\": \"Organization|Person|Product|Event|Location|Topic\",\n",
    "      \"aliases\": [\"Alternative Name 1\", \"Alternative Name 2\"],\n",
    "      \"confidence\": 0.95,\n",
    "      \"context\": \"The sentence or phrase where this entity was mentioned\",\n",
    "      \"properties\": {\n",
    "        \"additional_info\": \"any relevant details\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "Return only valid JSON, no additional text.\n",
    "\"\"\"\n",
    "\n",
    "def extract_entities_with_llm(content: str) -> List[Entity]:\n",
    "    \"\"\"Extract entities from content using LLM.\"\"\"\n",
    "    if not llm:\n",
    "        print(\"‚ö†Ô∏è LLM not initialized\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Create prompt\n",
    "        prompt = ENTITY_EXTRACTION_PROMPT.format(content=content)\n",
    "        \n",
    "        # Get LLM response\n",
    "        response = llm.invoke(prompt)\n",
    "        \n",
    "        # Parse JSON response\n",
    "        import json\n",
    "        result = json.loads(response.content)\n",
    "        \n",
    "        # Convert to Entity objects\n",
    "        entities = []\n",
    "        for entity_data in result.get('entities', []):\n",
    "            entity = Entity(\n",
    "                name=entity_data['name'],\n",
    "                type=entity_data['type'],\n",
    "                aliases=entity_data.get('aliases', []),\n",
    "                confidence=entity_data['confidence'],\n",
    "                context=entity_data.get('context'),\n",
    "                properties=entity_data.get('properties', {})\n",
    "            )\n",
    "            entities.append(entity)\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting entities: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test entity extraction (only if LLM is available)\n",
    "if llm:\n",
    "    print(\"üîç Testing entity extraction...\")\n",
    "    extracted_entities = extract_entities_with_llm(cleaned_text[:1000])  # Test with first 1000 chars\n",
    "    \n",
    "    if extracted_entities:\n",
    "        print(f\"‚úÖ Extracted {len(extracted_entities)} entities:\")\n",
    "        for i, entity in enumerate(extracted_entities[:5]):  # Show first 5\n",
    "            print(f\"  {i+1}. {entity.name} ({entity.type}) - Confidence: {entity.confidence}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No entities extracted\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping entity extraction test - LLM not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Neo4j Graph Database Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Neo4j connection established\n"
     ]
    }
   ],
   "source": [
    "# Neo4j Connection Manager\n",
    "class Neo4jConnection:\n",
    "    def __init__(self, uri: str, user: str, password: str):\n",
    "        self.uri = uri\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.driver = None\n",
    "        \n",
    "    def connect(self):\n",
    "        \"\"\"Establish connection to Neo4j.\"\"\"\n",
    "        try:\n",
    "            self.driver = GraphDatabase.driver(\n",
    "                self.uri, \n",
    "                auth=(self.user, self.password)\n",
    "            )\n",
    "            # Test connection\n",
    "            with self.driver.session() as session:\n",
    "                result = session.run(\"RETURN 1 as test\")\n",
    "                result.single()\n",
    "            print(\"‚úÖ Neo4j connection established\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Neo4j connection failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close Neo4j connection.\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.close()\n",
    "            print(\"üîå Neo4j connection closed\")\n",
    "    \n",
    "    def execute_query(self, query: str, parameters: dict = None):\n",
    "        \"\"\"Execute a Cypher query.\"\"\"\n",
    "        if not self.driver:\n",
    "            print(\"‚ùå No Neo4j connection\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            with self.driver.session() as session:\n",
    "                result = session.run(query, parameters or {})\n",
    "                return result.data()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Query execution failed: {e}\")\n",
    "            return None\n",
    "\n",
    "# Initialize Neo4j connection\n",
    "neo4j_conn = Neo4jConnection(\n",
    "    uri=config.NEO4J_URI,\n",
    "    user=config.NEO4J_USER,\n",
    "    password=config.NEO4J_PASSWORD\n",
    ")\n",
    "\n",
    "# Test connection\n",
    "connection_success = neo4j_conn.connect()\n",
    "if not connection_success:\n",
    "    print(\"‚ö†Ô∏è Neo4j connection failed. Please check your configuration.\")\n",
    "    print(\"   Make sure Neo4j is running and credentials are correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Setting up graph constraints and indexes...\n",
      "  ‚úÖ CONSTRAINT unique_org_name\n",
      "  ‚úÖ CONSTRAINT unique_person_name\n",
      "  ‚úÖ CONSTRAINT unique_product_name\n",
      "  ‚úÖ CONSTRAINT unique_event_name\n",
      "  ‚úÖ CONSTRAINT unique_location_name\n",
      "  ‚úÖ CONSTRAINT unique_topic_name\n",
      "  ‚úÖ CONSTRAINT unique_newsletter_id\n",
      "  ‚úÖ INDEX newsletter_date_idx\n",
      "  ‚úÖ INDEX entity_confidence_idx\n",
      "  ‚úÖ INDEX entity_last_seen_idx\n",
      "üìä Graph schema setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Graph database schema setup\n",
    "def setup_graph_constraints_and_indexes():\n",
    "    \"\"\"Create constraints and indexes for optimal graph performance.\"\"\"\n",
    "    if not neo4j_conn.driver:\n",
    "        print(\"‚ùå No Neo4j connection available\")\n",
    "        return\n",
    "    \n",
    "    constraints_and_indexes = [\n",
    "        # Unique constraints\n",
    "        \"CREATE CONSTRAINT unique_org_name IF NOT EXISTS FOR (o:Organization) REQUIRE o.name IS UNIQUE\",\n",
    "        \"CREATE CONSTRAINT unique_person_name IF NOT EXISTS FOR (p:Person) REQUIRE p.name IS UNIQUE\",\n",
    "        \"CREATE CONSTRAINT unique_product_name IF NOT EXISTS FOR (pr:Product) REQUIRE pr.name IS UNIQUE\",\n",
    "        \"CREATE CONSTRAINT unique_event_name IF NOT EXISTS FOR (e:Event) REQUIRE e.name IS UNIQUE\",\n",
    "        \"CREATE CONSTRAINT unique_location_name IF NOT EXISTS FOR (l:Location) REQUIRE l.name IS UNIQUE\",\n",
    "        \"CREATE CONSTRAINT unique_topic_name IF NOT EXISTS FOR (t:Topic) REQUIRE t.name IS UNIQUE\",\n",
    "        \"CREATE CONSTRAINT unique_newsletter_id IF NOT EXISTS FOR (n:Newsletter) REQUIRE n.id IS UNIQUE\",\n",
    "        \n",
    "        # Performance indexes\n",
    "        \"CREATE INDEX newsletter_date_idx IF NOT EXISTS FOR (n:Newsletter) ON (n.received_date)\",\n",
    "        \"CREATE INDEX entity_confidence_idx IF NOT EXISTS FOR (e:Organization) ON (e.confidence)\",\n",
    "        \"CREATE INDEX entity_last_seen_idx IF NOT EXISTS FOR (e:Organization) ON (e.last_seen)\",\n",
    "    ]\n",
    "    \n",
    "    print(\"üîß Setting up graph constraints and indexes...\")\n",
    "    \n",
    "    for constraint in constraints_and_indexes:\n",
    "        try:\n",
    "            neo4j_conn.execute_query(constraint)\n",
    "            print(f\"  ‚úÖ {constraint.split()[1]} {constraint.split()[2]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è {constraint.split()[1]} {constraint.split()[2]}: {e}\")\n",
    "    \n",
    "    print(\"üìä Graph schema setup complete!\")\n",
    "\n",
    "# Set up schema (only if connected)\n",
    "if connection_success:\n",
    "    setup_graph_constraints_and_indexes()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping schema setup - no Neo4j connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Graph Operations Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Current graph statistics:\n",
      "  Organizations: 0\n",
      "  People: 0\n",
      "  Products: 0\n",
      "  Events: 0\n",
      "  Locations: 0\n",
      "  Topics: 0\n",
      "  Newsletters: 0\n",
      "  Relationships: 0\n"
     ]
    }
   ],
   "source": [
    "# Graph operations for entity management\n",
    "class GraphOperations:\n",
    "    def __init__(self, neo4j_connection):\n",
    "        self.neo4j_conn = neo4j_connection\n",
    "    \n",
    "    def create_or_update_entity(self, entity: Entity) -> dict:\n",
    "        \"\"\"Create or update an entity node in the graph.\"\"\"\n",
    "        query = f\"\"\"\n",
    "        MERGE (e:{entity.type} {{name: $name}})\n",
    "        ON CREATE SET \n",
    "            e.created_at = datetime(),\n",
    "            e.confidence = $confidence,\n",
    "            e.aliases = $aliases,\n",
    "            e.mention_count = 1,\n",
    "            e.properties = $properties\n",
    "        ON MATCH SET\n",
    "            e.last_seen = datetime(),\n",
    "            e.mention_count = e.mention_count + 1,\n",
    "            e.confidence = CASE \n",
    "                WHEN $confidence > e.confidence THEN $confidence \n",
    "                ELSE e.confidence \n",
    "            END\n",
    "        RETURN e, \n",
    "               CASE WHEN e.created_at = e.last_seen THEN 'created' ELSE 'updated' END as operation\n",
    "        \"\"\"\n",
    "        \n",
    "        parameters = {\n",
    "            'name': entity.name,\n",
    "            'confidence': entity.confidence,\n",
    "            'aliases': entity.aliases,\n",
    "            'properties': entity.properties\n",
    "        }\n",
    "        \n",
    "        result = self.neo4j_conn.execute_query(query, parameters)\n",
    "        return result[0] if result else None\n",
    "    \n",
    "    def create_newsletter_node(self, newsletter: Newsletter) -> dict:\n",
    "        \"\"\"Create a newsletter node in the graph.\"\"\"\n",
    "        query = \"\"\"\n",
    "        CREATE (n:Newsletter {\n",
    "            id: $newsletter_id,\n",
    "            subject: $subject,\n",
    "            sender: $sender,\n",
    "            received_date: $received_date,\n",
    "            created_at: datetime(),\n",
    "            content_length: $content_length\n",
    "        })\n",
    "        RETURN n\n",
    "        \"\"\"\n",
    "        \n",
    "        parameters = {\n",
    "            'newsletter_id': newsletter.newsletter_id,\n",
    "            'subject': newsletter.subject,\n",
    "            'sender': newsletter.sender,\n",
    "            'received_date': newsletter.received_date,\n",
    "            'content_length': len(newsletter.html_content)\n",
    "        }\n",
    "        \n",
    "        result = self.neo4j_conn.execute_query(query, parameters)\n",
    "        return result[0] if result else None\n",
    "    \n",
    "    def link_entity_to_newsletter(self, entity_name: str, entity_type: str, newsletter_id: str, context: str = None) -> dict:\n",
    "        \"\"\"Create a MENTIONED_IN relationship between entity and newsletter.\"\"\"\n",
    "        query = f\"\"\"\n",
    "        MATCH (e:{entity_type} {{name: $entity_name}})\n",
    "        MATCH (n:Newsletter {{id: $newsletter_id}})\n",
    "        CREATE (e)-[r:MENTIONED_IN {{\n",
    "            date: datetime(),\n",
    "            context: $context\n",
    "        }}]->(n)\n",
    "        RETURN r\n",
    "        \"\"\"\n",
    "        \n",
    "        parameters = {\n",
    "            'entity_name': entity_name,\n",
    "            'newsletter_id': newsletter_id,\n",
    "            'context': context\n",
    "        }\n",
    "        \n",
    "        result = self.neo4j_conn.execute_query(query, parameters)\n",
    "        return result[0] if result else None\n",
    "    \n",
    "    def find_similar_entities(self, entity_name: str, entity_type: str, similarity_threshold: float = 0.8) -> List[dict]:\n",
    "        \"\"\"Find entities with similar names for resolution.\"\"\"\n",
    "        query = f\"\"\"\n",
    "        MATCH (e:{entity_type})\n",
    "        WHERE e.name CONTAINS $search_term \n",
    "           OR ANY(alias IN e.aliases WHERE alias CONTAINS $search_term)\n",
    "           OR $search_term CONTAINS e.name\n",
    "        RETURN e, \n",
    "               e.mention_count as popularity,\n",
    "               e.confidence as confidence\n",
    "        ORDER BY popularity DESC, confidence DESC\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "        \n",
    "        parameters = {'search_term': entity_name}\n",
    "        result = self.neo4j_conn.execute_query(query, parameters)\n",
    "        return result or []\n",
    "    \n",
    "    def get_graph_stats(self) -> dict:\n",
    "        \"\"\"Get basic statistics about the graph.\"\"\"\n",
    "        stats_query = \"\"\"\n",
    "        CALL {\n",
    "            MATCH (o:Organization) RETURN count(o) as organizations\n",
    "        }\n",
    "        CALL {\n",
    "            MATCH (p:Person) RETURN count(p) as people\n",
    "        }\n",
    "        CALL {\n",
    "            MATCH (pr:Product) RETURN count(pr) as products\n",
    "        }\n",
    "        CALL {\n",
    "            MATCH (e:Event) RETURN count(e) as events\n",
    "        }\n",
    "        CALL {\n",
    "            MATCH (l:Location) RETURN count(l) as locations\n",
    "        }\n",
    "        CALL {\n",
    "            MATCH (t:Topic) RETURN count(t) as topics\n",
    "        }\n",
    "        CALL {\n",
    "            MATCH (n:Newsletter) RETURN count(n) as newsletters\n",
    "        }\n",
    "        CALL {\n",
    "            MATCH ()-[r]->() RETURN count(r) as relationships\n",
    "        }\n",
    "        RETURN organizations, people, products, events, locations, topics, newsletters, relationships\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self.neo4j_conn.execute_query(stats_query)\n",
    "        return result[0] if result else {}\n",
    "\n",
    "# Initialize graph operations\n",
    "if connection_success:\n",
    "    graph_ops = GraphOperations(neo4j_conn)\n",
    "    \n",
    "    # Get initial graph statistics\n",
    "    stats = graph_ops.get_graph_stats()\n",
    "    print(\"üìä Current graph statistics:\")\n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key.capitalize()}: {value}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Graph operations not available - no Neo4j connection\")\n",
    "    graph_ops = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Complete Processing Pipeline Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Testing complete newsletter processing pipeline...\n",
      "üöÄ Starting newsletter processing: AI Weekly Newsletter #245\n",
      "  1Ô∏è‚É£ Cleaning HTML content...\n",
      "  2Ô∏è‚É£ Extracting entities with LLM...\n",
      "‚ùå Error extracting entities: '\\n  \"entities\"'\n",
      "    ‚úÖ Extracted 0 entities\n",
      "  3Ô∏è‚É£ Creating newsletter node...\n",
      "    ‚úÖ Newsletter node created\n",
      "  4Ô∏è‚É£ Processing entities in graph...\n",
      "  5Ô∏è‚É£ Generating summary...\n",
      "‚úÖ Processing completed in 0.21 seconds\n",
      "\n",
      "üìä Pipeline Results:\n",
      "  Status: success\n",
      "  Processing time: 0.21 seconds\n",
      "  Entities extracted: 0\n",
      "  New entities: 0\n",
      "  Updated entities: 0\n",
      "  Entity summary: {'Organization': 0, 'Person': 0, 'Product': 0, 'Event': 0, 'Location': 0, 'Topic': 0}\n",
      "\n",
      "üìù Text Summary:\n",
      "Processed newsletter 'AI Weekly Newsletter #245' from ai-weekly@example.com. \n",
      "Extracted 0 entities (none). \n",
      "Created 0 new entities, updated 0 existing entities. \n",
      "Processing completed in 0.21 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Complete processing pipeline function\n",
    "def process_newsletter_pipeline(newsletter: Newsletter) -> dict:\n",
    "    \"\"\"Process a newsletter through the complete pipeline.\"\"\"\n",
    "    start_time = datetime.now()\n",
    "    results = {\n",
    "        'status': 'success',\n",
    "        'processing_time': 0,\n",
    "        'entities_extracted': 0,\n",
    "        'entities_new': 0,\n",
    "        'entities_updated': 0,\n",
    "        'newsletter_node_id': newsletter.newsletter_id,\n",
    "        'summary': {},\n",
    "        'text_summary': '',\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"üöÄ Starting newsletter processing: {newsletter.subject}\")\n",
    "        \n",
    "        # Step 1: Clean HTML content\n",
    "        print(\"  1Ô∏è‚É£ Cleaning HTML content...\")\n",
    "        cleaned_text = clean_html_content(newsletter.html_content)\n",
    "        if not cleaned_text:\n",
    "            results['errors'].append(\"Failed to clean HTML content\")\n",
    "            results['status'] = 'error'\n",
    "            return results\n",
    "        \n",
    "        # Step 2: Extract entities with LLM\n",
    "        print(\"  2Ô∏è‚É£ Extracting entities with LLM...\")\n",
    "        if llm:\n",
    "            entities = extract_entities_with_llm(cleaned_text)\n",
    "            results['entities_extracted'] = len(entities)\n",
    "            print(f\"    ‚úÖ Extracted {len(entities)} entities\")\n",
    "        else:\n",
    "            entities = []\n",
    "            results['errors'].append(\"LLM not available for entity extraction\")\n",
    "        \n",
    "        # Step 3: Create newsletter node in graph\n",
    "        print(\"  3Ô∏è‚É£ Creating newsletter node...\")\n",
    "        if graph_ops:\n",
    "            newsletter_node = graph_ops.create_newsletter_node(newsletter)\n",
    "            if newsletter_node:\n",
    "                print(\"    ‚úÖ Newsletter node created\")\n",
    "            else:\n",
    "                results['errors'].append(\"Failed to create newsletter node\")\n",
    "        else:\n",
    "            results['errors'].append(\"Graph operations not available\")\n",
    "        \n",
    "        # Step 4: Process entities and create/update nodes\n",
    "        print(\"  4Ô∏è‚É£ Processing entities in graph...\")\n",
    "        entity_summary = {'Organization': 0, 'Person': 0, 'Product': 0, 'Event': 0, 'Location': 0, 'Topic': 0}\n",
    "        new_entities = 0\n",
    "        updated_entities = 0\n",
    "        \n",
    "        if graph_ops and entities:\n",
    "            for entity in entities:\n",
    "                try:\n",
    "                    # Create or update entity\n",
    "                    result = graph_ops.create_or_update_entity(entity)\n",
    "                    if result:\n",
    "                        if result.get('operation') == 'created':\n",
    "                            new_entities += 1\n",
    "                        else:\n",
    "                            updated_entities += 1\n",
    "                        \n",
    "                        # Link to newsletter\n",
    "                        graph_ops.link_entity_to_newsletter(\n",
    "                            entity.name, \n",
    "                            entity.type, \n",
    "                            newsletter.newsletter_id, \n",
    "                            entity.context\n",
    "                        )\n",
    "                        \n",
    "                        # Update summary\n",
    "                        entity_summary[entity.type] += 1\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    results['errors'].append(f\"Error processing entity {entity.name}: {e}\")\n",
    "            \n",
    "            print(f\"    ‚úÖ Created {new_entities} new entities, updated {updated_entities} existing entities\")\n",
    "        \n",
    "        # Step 5: Generate summary\n",
    "        print(\"  5Ô∏è‚É£ Generating summary...\")\n",
    "        results['entities_new'] = new_entities\n",
    "        results['entities_updated'] = updated_entities\n",
    "        results['summary'] = entity_summary\n",
    "        \n",
    "        # Create text summary\n",
    "        entity_mentions = []\n",
    "        for entity_type, count in entity_summary.items():\n",
    "            if count > 0:\n",
    "                entity_mentions.append(f\"{count} {entity_type.lower()}{'s' if count > 1 else ''}\")\n",
    "        \n",
    "        processing_time = (datetime.now() - start_time).total_seconds()\n",
    "        results['processing_time'] = processing_time\n",
    "        \n",
    "        results['text_summary'] = f\"\"\"Processed newsletter '{newsletter.subject}' from {newsletter.sender}. \n",
    "Extracted {results['entities_extracted']} entities ({', '.join(entity_mentions) if entity_mentions else 'none'}). \n",
    "Created {new_entities} new entities, updated {updated_entities} existing entities. \n",
    "Processing completed in {processing_time:.2f} seconds.\"\"\"\n",
    "        \n",
    "        print(f\"‚úÖ Processing completed in {processing_time:.2f} seconds\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        results['status'] = 'error'\n",
    "        results['errors'].append(f\"Pipeline error: {e}\")\n",
    "        results['processing_time'] = (datetime.now() - start_time).total_seconds()\n",
    "        print(f\"‚ùå Processing failed: {e}\")\n",
    "        return results\n",
    "\n",
    "# Test the complete pipeline\n",
    "print(\"üîÑ Testing complete newsletter processing pipeline...\")\n",
    "pipeline_results = process_newsletter_pipeline(sample_newsletter)\n",
    "\n",
    "print(\"\\nüìä Pipeline Results:\")\n",
    "print(f\"  Status: {pipeline_results['status']}\")\n",
    "print(f\"  Processing time: {pipeline_results['processing_time']:.2f} seconds\")\n",
    "print(f\"  Entities extracted: {pipeline_results['entities_extracted']}\")\n",
    "print(f\"  New entities: {pipeline_results['entities_new']}\")\n",
    "print(f\"  Updated entities: {pipeline_results['entities_updated']}\")\n",
    "print(f\"  Entity summary: {pipeline_results['summary']}\")\n",
    "\n",
    "if pipeline_results['errors']:\n",
    "    print(f\"  Errors: {pipeline_results['errors']}\")\n",
    "\n",
    "print(f\"\\nüìù Text Summary:\")\n",
    "print(pipeline_results['text_summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Updated graph statistics:\n",
      "  Organizations: 0\n",
      "  People: 0\n",
      "  Products: 0\n",
      "  Events: 0\n",
      "  Locations: 0\n",
      "  Topics: 0\n",
      "  Newsletters: 1\n",
      "  Relationships: 0\n",
      "üìä No entities to visualize\n"
     ]
    }
   ],
   "source": [
    "# Analyze and visualize results\n",
    "if graph_ops and pipeline_results['status'] == 'success':\n",
    "    # Get updated graph statistics\n",
    "    updated_stats = graph_ops.get_graph_stats()\n",
    "    \n",
    "    print(\"üìä Updated graph statistics:\")\n",
    "    for key, value in updated_stats.items():\n",
    "        print(f\"  {key.capitalize()}: {value}\")\n",
    "    \n",
    "    # Create visualization of entity types\n",
    "    entity_data = pipeline_results['summary']\n",
    "    entity_types = list(entity_data.keys())\n",
    "    entity_counts = list(entity_data.values())\n",
    "    \n",
    "    if sum(entity_counts) > 0:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Bar chart of entity types\n",
    "        bars = ax1.bar(entity_types, entity_counts, color=sns.color_palette(\"husl\", len(entity_types)))\n",
    "        ax1.set_title('Extracted Entities by Type')\n",
    "        ax1.set_xlabel('Entity Type')\n",
    "        ax1.set_ylabel('Count')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, count in zip(bars, entity_counts):\n",
    "            if count > 0:\n",
    "                ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                        str(count), ha='center', va='bottom')\n",
    "        \n",
    "        # Pie chart of entity distribution\n",
    "        non_zero_types = [t for t, c in zip(entity_types, entity_counts) if c > 0]\n",
    "        non_zero_counts = [c for c in entity_counts if c > 0]\n",
    "        \n",
    "        if non_zero_counts:\n",
    "            ax2.pie(non_zero_counts, labels=non_zero_types, autopct='%1.1f%%', startangle=90)\n",
    "            ax2.set_title('Entity Distribution')\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, 'No entities extracted', ha='center', va='center', transform=ax2.transAxes)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"üìä No entities to visualize\")\n",
    "else:\n",
    "    print(\"üìä Visualization skipped - no successful results or graph connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps and Production Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development Summary\n",
    "\n",
    "This notebook has successfully implemented and tested the core components of the newsletter processing pipeline:\n",
    "\n",
    "‚úÖ **Completed Components:**\n",
    "- Configuration management with environment variables\n",
    "- HTML processing and text extraction\n",
    "- LLM integration for entity extraction\n",
    "- Neo4j graph database connection and operations\n",
    "- Entity creation and relationship management\n",
    "- Complete processing pipeline\n",
    "- Results analysis and visualization\n",
    "\n",
    "### Next Development Steps:\n",
    "\n",
    "1. **Fact Extraction**: Implement relationship extraction between entities\n",
    "2. **Temporal Processing**: Add time-based relationship handling\n",
    "3. **Entity Resolution**: Improve fuzzy matching and disambiguation\n",
    "4. **Batch Processing**: Handle multiple newsletters efficiently\n",
    "5. **Error Handling**: Add comprehensive error recovery\n",
    "6. **Performance Optimization**: Optimize LLM calls and graph operations\n",
    "\n",
    "### Production Migration:\n",
    "\n",
    "1. **Extract Code to Modules**: Move notebook functions to `src/` directory\n",
    "2. **FastAPI Integration**: Create REST endpoint using developed logic\n",
    "3. **Testing**: Implement comprehensive test suite\n",
    "4. **Deployment**: Configure for Cloud Run deployment\n",
    "5. **Monitoring**: Add logging and metrics collection\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "- The pipeline successfully processes newsletter content end-to-end\n",
    "- Entity extraction quality depends on LLM prompt engineering\n",
    "- Graph operations are efficient for entity management\n",
    "- Real-time processing is feasible for typical newsletter volumes\n",
    "- Error handling is critical for production reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîå Neo4j connection closed\n",
      "üßπ Cleanup completed\n",
      "\n",
      "üéâ Newsletter processing pipeline development complete!\n",
      "\n",
      "üìã Ready for production migration:\n",
      "  1. Extract functions to src/ modules\n",
      "  2. Create FastAPI endpoints\n",
      "  3. Add comprehensive testing\n",
      "  4. Configure deployment\n",
      "  5. Set up monitoring\n"
     ]
    }
   ],
   "source": [
    "# Clean up connections\n",
    "if neo4j_conn:\n",
    "    neo4j_conn.close()\n",
    "\n",
    "print(\"üßπ Cleanup completed\")\n",
    "print(\"\\nüéâ Newsletter processing pipeline development complete!\")\n",
    "print(\"\\nüìã Ready for production migration:\")\n",
    "print(\"  1. Extract functions to src/ modules\")\n",
    "print(\"  2. Create FastAPI endpoints\")\n",
    "print(\"  3. Add comprehensive testing\")\n",
    "print(\"  4. Configure deployment\")\n",
    "print(\"  5. Set up monitoring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5ru2ky5ahb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment check:\n",
      "NEO4J_URI: bolt://localhost:7687\n",
      "NEO4J_USER: neo4j\n",
      "NEO4J_PASSWORD: Set\n"
     ]
    }
   ],
   "source": [
    "# Test if we can run Neo4j code in the notebook kernel\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(Path(\"..\") / \".env\")\n",
    "\n",
    "print(\"Environment check:\")\n",
    "print(f\"NEO4J_URI: {os.getenv('NEO4J_URI', 'Not set')}\")\n",
    "print(f\"NEO4J_USER: {os.getenv('NEO4J_USER', 'Not set')}\")\n",
    "print(f\"NEO4J_PASSWORD: {'Set' if os.getenv('NEO4J_PASSWORD') else 'Not set'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "paegdtlh91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Neo4j connection established\n",
      "üéâ Step 6 should work now!\n",
      "üîå Neo4j connection closed\n"
     ]
    }
   ],
   "source": [
    "# Test Neo4j connection in notebook kernel\n",
    "class Neo4jConnection:\n",
    "    def __init__(self, uri: str, user: str, password: str):\n",
    "        self.uri = uri\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.driver = None\n",
    "        \n",
    "    def connect(self):\n",
    "        \"\"\"Establish connection to Neo4j.\"\"\"\n",
    "        try:\n",
    "            self.driver = GraphDatabase.driver(\n",
    "                self.uri, \n",
    "                auth=(self.user, self.password)\n",
    "            )\n",
    "            # Test connection\n",
    "            with self.driver.session() as session:\n",
    "                result = session.run(\"RETURN 1 as test\")\n",
    "                result.single()\n",
    "            print(\"‚úÖ Neo4j connection established\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Neo4j connection failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close Neo4j connection.\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.close()\n",
    "            print(\"üîå Neo4j connection closed\")\n",
    "\n",
    "# Test connection\n",
    "neo4j_conn = Neo4jConnection(\n",
    "    uri=os.getenv(\"NEO4J_URI\", \"bolt://localhost:7687\"),\n",
    "    user=os.getenv(\"NEO4J_USER\", \"neo4j\"),\n",
    "    password=os.getenv(\"NEO4J_PASSWORD\")\n",
    ")\n",
    "\n",
    "connection_success = neo4j_conn.connect()\n",
    "if not connection_success:\n",
    "    print(\"‚ö†Ô∏è Neo4j connection failed. Please check your configuration.\")\n",
    "    print(\"   Make sure Neo4j is running and credentials are correct.\")\n",
    "else:\n",
    "    print(\"üéâ Step 6 should work now!\")\n",
    "    neo4j_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dlbrd5hpz6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Neo4j connection established\n",
      "üîß Setting up graph constraints and indexes...\n",
      "  ‚úÖ CONSTRAINT unique_org_name\n",
      "  ‚úÖ CONSTRAINT unique_person_name\n",
      "  ‚úÖ CONSTRAINT unique_product_name\n",
      "  ‚úÖ CONSTRAINT unique_event_name\n",
      "  ‚úÖ CONSTRAINT unique_location_name\n",
      "  ‚úÖ CONSTRAINT unique_topic_name\n",
      "  ‚úÖ CONSTRAINT unique_newsletter_id\n",
      "  ‚úÖ INDEX newsletter_date_idx\n",
      "  ‚úÖ INDEX entity_confidence_idx\n",
      "  ‚úÖ INDEX entity_last_seen_idx\n",
      "üìä Graph schema setup complete!\n",
      "üîå Neo4j connection closed\n"
     ]
    }
   ],
   "source": [
    "# Test the schema setup function from the notebook\n",
    "def setup_graph_constraints_and_indexes():\n",
    "    \"\"\"Create constraints and indexes for optimal graph performance.\"\"\"\n",
    "    if not neo4j_conn.driver:\n",
    "        print(\"‚ùå No Neo4j connection available\")\n",
    "        return\n",
    "    \n",
    "    constraints_and_indexes = [\n",
    "        # Unique constraints\n",
    "        \"CREATE CONSTRAINT unique_org_name IF NOT EXISTS FOR (o:Organization) REQUIRE o.name IS UNIQUE\",\n",
    "        \"CREATE CONSTRAINT unique_person_name IF NOT EXISTS FOR (p:Person) REQUIRE p.name IS UNIQUE\",\n",
    "        \"CREATE CONSTRAINT unique_product_name IF NOT EXISTS FOR (pr:Product) REQUIRE pr.name IS UNIQUE\",\n",
    "        \"CREATE CONSTRAINT unique_event_name IF NOT EXISTS FOR (e:Event) REQUIRE e.name IS UNIQUE\",\n",
    "        \"CREATE CONSTRAINT unique_location_name IF NOT EXISTS FOR (l:Location) REQUIRE l.name IS UNIQUE\",\n",
    "        \"CREATE CONSTRAINT unique_topic_name IF NOT EXISTS FOR (t:Topic) REQUIRE t.name IS UNIQUE\",\n",
    "        \"CREATE CONSTRAINT unique_newsletter_id IF NOT EXISTS FOR (n:Newsletter) REQUIRE n.id IS UNIQUE\",\n",
    "        \n",
    "        # Performance indexes\n",
    "        \"CREATE INDEX newsletter_date_idx IF NOT EXISTS FOR (n:Newsletter) ON (n.received_date)\",\n",
    "        \"CREATE INDEX entity_confidence_idx IF NOT EXISTS FOR (e:Organization) ON (e.confidence)\",\n",
    "        \"CREATE INDEX entity_last_seen_idx IF NOT EXISTS FOR (e:Organization) ON (e.last_seen)\",\n",
    "    ]\n",
    "    \n",
    "    print(\"üîß Setting up graph constraints and indexes...\")\n",
    "    \n",
    "    def execute_query(query: str, parameters: dict = None):\n",
    "        \"\"\"Execute a Cypher query.\"\"\"\n",
    "        try:\n",
    "            with neo4j_conn.driver.session() as session:\n",
    "                result = session.run(query, parameters or {})\n",
    "                return result.data()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Query execution failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    for constraint in constraints_and_indexes:\n",
    "        try:\n",
    "            execute_query(constraint)\n",
    "            constraint_type = constraint.split()[1]\n",
    "            constraint_name = constraint.split()[2]\n",
    "            print(f\"  ‚úÖ {constraint_type} {constraint_name}\")\n",
    "        except Exception as e:\n",
    "            constraint_type = constraint.split()[1]\n",
    "            constraint_name = constraint.split()[2]\n",
    "            print(f\"  ‚ö†Ô∏è {constraint_type} {constraint_name}: {e}\")\n",
    "    \n",
    "    print(\"üìä Graph schema setup complete!\")\n",
    "\n",
    "# Re-establish connection for testing\n",
    "neo4j_conn.connect()\n",
    "\n",
    "# Test schema setup\n",
    "setup_graph_constraints_and_indexes()\n",
    "\n",
    "neo4j_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "g1ngj1b6te6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fixed entity extraction function defined\n"
     ]
    }
   ],
   "source": [
    "# Let's test and fix the entity extraction function\n",
    "import json\n",
    "import re\n",
    "\n",
    "# First, let's test what the LLM is actually returning\n",
    "def extract_entities_with_llm_fixed(content: str):\n",
    "    \"\"\"Extract entities from content using LLM with improved JSON parsing.\"\"\"\n",
    "    # Check if LLM is available (this would be set in previous cells)\n",
    "    try:\n",
    "        # For testing, let's see if we have the llm object\n",
    "        if 'llm' not in globals():\n",
    "            print(\"‚ö†Ô∏è LLM not initialized\")\n",
    "            return []\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = ENTITY_EXTRACTION_PROMPT.format(content=content)\n",
    "        \n",
    "        # Get LLM response\n",
    "        response = llm.invoke(prompt)\n",
    "        \n",
    "        print(f\"Raw LLM response: {repr(response.content)}\")\n",
    "        \n",
    "        # Try to extract JSON from the response\n",
    "        response_text = response.content.strip()\n",
    "        \n",
    "        # Look for JSON block markers and extract content\n",
    "        if '```json' in response_text:\n",
    "            # Extract content between ```json and ```\n",
    "            json_match = re.search(r'```json\\s*(.*?)\\s*```', response_text, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_text = json_match.group(1).strip()\n",
    "            else:\n",
    "                json_text = response_text\n",
    "        elif '```' in response_text:\n",
    "            # Extract content between any ``` blocks\n",
    "            json_match = re.search(r'```\\s*(.*?)\\s*```', response_text, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_text = json_match.group(1).strip()\n",
    "            else:\n",
    "                json_text = response_text\n",
    "        else:\n",
    "            json_text = response_text\n",
    "        \n",
    "        print(f\"Extracted JSON text: {repr(json_text)}\")\n",
    "        \n",
    "        # Parse JSON response\n",
    "        try:\n",
    "            result = json.loads(json_text)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON decode error: {e}\")\n",
    "            # Try to fix common issues\n",
    "            # Remove any leading/trailing non-JSON content\n",
    "            lines = json_text.split('\\n')\n",
    "            json_lines = []\n",
    "            in_json = False\n",
    "            for line in lines:\n",
    "                if line.strip().startswith('{') or in_json:\n",
    "                    in_json = True\n",
    "                    json_lines.append(line)\n",
    "                    if line.strip().endswith('}') and line.count('}') >= line.count('{'):\n",
    "                        break\n",
    "            \n",
    "            if json_lines:\n",
    "                json_text = '\\n'.join(json_lines)\n",
    "                print(f\"Cleaned JSON: {repr(json_text)}\")\n",
    "                result = json.loads(json_text)\n",
    "            else:\n",
    "                raise e\n",
    "        \n",
    "        # Convert to Entity objects\n",
    "        from pydantic import BaseModel, Field\n",
    "        from typing import List, Dict, Any, Optional\n",
    "        \n",
    "        class Entity(BaseModel):\n",
    "            name: str\n",
    "            type: str\n",
    "            aliases: List[str] = Field(default_factory=list)\n",
    "            confidence: float = Field(ge=0.0, le=1.0)\n",
    "            context: Optional[str] = None\n",
    "            properties: Dict[str, Any] = Field(default_factory=dict)\n",
    "        \n",
    "        entities = []\n",
    "        for entity_data in result.get('entities', []):\n",
    "            entity = Entity(\n",
    "                name=entity_data['name'],\n",
    "                type=entity_data['type'],\n",
    "                aliases=entity_data.get('aliases', []),\n",
    "                confidence=entity_data['confidence'],\n",
    "                context=entity_data.get('context'),\n",
    "                properties=entity_data.get('properties', {})\n",
    "            )\n",
    "            entities.append(entity)\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting entities: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"‚úÖ Fixed entity extraction function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bgmlhivlgyp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking LLM configuration:\n",
      "OPENAI_API_KEY: Set\n",
      "‚úÖ LLM initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Check if we have the necessary variables from previous cells\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(Path(\"..\") / \".env\")\n",
    "\n",
    "print(\"Checking LLM configuration:\")\n",
    "print(f\"OPENAI_API_KEY: {'Set' if os.getenv('OPENAI_API_KEY') else 'Not set'}\")\n",
    "\n",
    "# Check if we can initialize LLM\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    \n",
    "    if os.getenv('OPENAI_API_KEY'):\n",
    "        openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "        llm = ChatOpenAI(\n",
    "            model=os.getenv('LLM_MODEL', 'gpt-4-turbo'),\n",
    "            temperature=float(os.getenv('LLM_TEMPERATURE', '0.1')),\n",
    "            max_tokens=int(os.getenv('LLM_MAX_TOKENS', '2000')),\n",
    "            openai_api_key=os.getenv('OPENAI_API_KEY')\n",
    "        )\n",
    "        print(\"‚úÖ LLM initialized successfully\")\n",
    "    else:\n",
    "        print(\"‚ùå OpenAI API key not found\")\n",
    "        llm = None\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå LLM initialization failed: {e}\")\n",
    "    llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "u4jpz98sgcq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing entity extraction...\n",
      "Test content: OpenAI announced GPT-4 at their conference in San Francisco. CEO Sam Altman presented the new AI model.\n",
      "\n",
      "Raw LLM response:\n",
      "'```json\\n{\\n  \"entities\": [\\n    {\\n      \"name\": \"OpenAI\",\\n      \"type\": \"Organization\",\\n      \"aliases\": [],\\n      \"confidence\": 0.99,\\n      \"context\": \"OpenAI announced GPT-4 at their conference in San Francisco.\",\\n      \"properties\": {\\n        \"additional_info\": \"AI research organization\"\\n      }\\n    },\\n    {\\n      \"name\": \"GPT-4\",\\n      \"type\": \"Product\",\\n      \"aliases\": [],\\n      \"confidence\": 0.98,\\n      \"context\": \"OpenAI announced GPT-4 at their conference in San Francisco.\",\\n      \"properties\": {\\n        \"additional_info\": \"AI model\"\\n      }\\n    },\\n    {\\n      \"name\": \"San Francisco\",\\n      \"type\": \"Location\",\\n      \"aliases\": [],\\n      \"confidence\": 0.95,\\n      \"context\": \"OpenAI announced GPT-4 at their conference in San Francisco.\",\\n      \"properties\": {\\n        \"additional_info\": \"City in California, USA\"\\n      }\\n    },\\n    {\\n      \"name\": \"Sam Altman\",\\n      \"type\": \"Person\",\\n      \"aliases\": [],\\n      \"confidence\": 0.95,\\n      \"context\": \"CEO Sam Altman presented the new AI model.\",\\n      \"properties\": {\\n        \"additional_info\": \"CEO of OpenAI\"\\n      }\\n    }\\n  ]\\n}\\n```'\n",
      "\n",
      "‚ùå JSON parsing failed: Expecting value: line 1 column 1 (char 0)\n",
      "Trying to extract JSON from response...\n",
      "Extracted JSON: '{\\n  \"entities\": [\\n    {\\n      \"name\": \"OpenAI\",\\n      \"type\": \"Organization\",\\n      \"aliases\": [],\\n      \"confidence\": 0.99,\\n      \"context\": \"OpenAI announced GPT-4 at their conference in San Francisco.\",\\n      \"properties\": {\\n        \"additional_info\": \"AI research organization\"\\n      }\\n    },\\n    {\\n      \"name\": \"GPT-4\",\\n      \"type\": \"Product\",\\n      \"aliases\": [],\\n      \"confidence\": 0.98,\\n      \"context\": \"OpenAI announced GPT-4 at their conference in San Francisco.\",\\n      \"properties\": {\\n        \"additional_info\": \"AI model\"\\n      }\\n    },\\n    {\\n      \"name\": \"San Francisco\",\\n      \"type\": \"Location\",\\n      \"aliases\": [],\\n      \"confidence\": 0.95,\\n      \"context\": \"OpenAI announced GPT-4 at their conference in San Francisco.\",\\n      \"properties\": {\\n        \"additional_info\": \"City in California, USA\"\\n      }\\n    },\\n    {\\n      \"name\": \"Sam Altman\",\\n      \"type\": \"Person\",\\n      \"aliases\": [],\\n      \"confidence\": 0.95,\\n      \"context\": \"CEO Sam Altman presented the new AI model.\",\\n      \"properties\": {\\n        \"additional_info\": \"CEO of OpenAI\"\\n      }\\n    }\\n  ]\\n}'\n",
      "\n",
      "‚úÖ JSON extracted and parsed:\n",
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"name\": \"OpenAI\",\n",
      "      \"type\": \"Organization\",\n",
      "      \"aliases\": [],\n",
      "      \"confidence\": 0.99,\n",
      "      \"context\": \"OpenAI announced GPT-4 at their conference in San Francisco.\",\n",
      "      \"properties\": {\n",
      "        \"additional_info\": \"AI research organization\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"GPT-4\",\n",
      "      \"type\": \"Product\",\n",
      "      \"aliases\": [],\n",
      "      \"confidence\": 0.98,\n",
      "      \"context\": \"OpenAI announced GPT-4 at their conference in San Francisco.\",\n",
      "      \"properties\": {\n",
      "        \"additional_info\": \"AI model\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"San Francisco\",\n",
      "      \"type\": \"Location\",\n",
      "      \"aliases\": [],\n",
      "      \"confidence\": 0.95,\n",
      "      \"context\": \"OpenAI announced GPT-4 at their conference in San Francisco.\",\n",
      "      \"properties\": {\n",
      "        \"additional_info\": \"City in California, USA\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Sam Altman\",\n",
      "      \"type\": \"Person\",\n",
      "      \"aliases\": [],\n",
      "      \"confidence\": 0.95,\n",
      "      \"context\": \"CEO Sam Altman presented the new AI model.\",\n",
      "      \"properties\": {\n",
      "        \"additional_info\": \"CEO of OpenAI\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test entity extraction with a simple example\n",
    "ENTITY_EXTRACTION_PROMPT = \"\"\"\n",
    "You are an expert at extracting structured information from newsletter content. \n",
    "Extract entities from the following newsletter text and classify them into these categories:\n",
    "\n",
    "**Entity Types:**\n",
    "- **Organization**: Companies, institutions, government bodies\n",
    "- **Person**: Individuals mentioned in content\n",
    "- **Product**: Software, hardware, services, models\n",
    "- **Event**: Conferences, announcements, launches\n",
    "- **Location**: Geographic locations (cities, countries, regions)\n",
    "- **Topic**: Subject areas, technologies, fields of study\n",
    "\n",
    "**Instructions:**\n",
    "1. Extract entities with high confidence (>0.7)\n",
    "2. Provide alternative names/aliases if mentioned\n",
    "3. Include context where the entity was mentioned\n",
    "4. Rate confidence from 0.0 to 1.0\n",
    "5. Return results as valid JSON\n",
    "\n",
    "**Newsletter Content:**\n",
    "{content}\n",
    "\n",
    "**Required JSON Format:**\n",
    "```json\n",
    "{{\n",
    "  \"entities\": [\n",
    "    {{\n",
    "      \"name\": \"Entity Name\",\n",
    "      \"type\": \"Organization|Person|Product|Event|Location|Topic\",\n",
    "      \"aliases\": [\"Alternative Name 1\", \"Alternative Name 2\"],\n",
    "      \"confidence\": 0.95,\n",
    "      \"context\": \"The sentence or phrase where this entity was mentioned\",\n",
    "      \"properties\": {{\n",
    "        \"additional_info\": \"any relevant details\"\n",
    "      }}\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "```\n",
    "\n",
    "Return only valid JSON, no additional text.\n",
    "\"\"\"\n",
    "\n",
    "# Test with simple content\n",
    "test_content = \"OpenAI announced GPT-4 at their conference in San Francisco. CEO Sam Altman presented the new AI model.\"\n",
    "\n",
    "print(\"Testing entity extraction...\")\n",
    "print(f\"Test content: {test_content}\")\n",
    "\n",
    "try:\n",
    "    # Create prompt\n",
    "    prompt = ENTITY_EXTRACTION_PROMPT.format(content=test_content)\n",
    "    \n",
    "    # Get LLM response\n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    print(f\"\\nRaw LLM response:\")\n",
    "    print(repr(response.content))\n",
    "    \n",
    "    # Try to parse as JSON\n",
    "    import json\n",
    "    try:\n",
    "        result = json.loads(response.content)\n",
    "        print(f\"\\n‚úÖ JSON parsed successfully:\")\n",
    "        print(json.dumps(result, indent=2))\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"\\n‚ùå JSON parsing failed: {e}\")\n",
    "        print(\"Trying to extract JSON from response...\")\n",
    "        \n",
    "        # Extract JSON using regex\n",
    "        import re\n",
    "        json_match = re.search(r'```json\\s*(.*?)\\s*```', response.content, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_text = json_match.group(1).strip()\n",
    "            print(f\"Extracted JSON: {repr(json_text)}\")\n",
    "            result = json.loads(json_text)\n",
    "            print(f\"\\n‚úÖ JSON extracted and parsed:\")\n",
    "            print(json.dumps(result, indent=2))\n",
    "        else:\n",
    "            print(\"No JSON block found in response\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during entity extraction test: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "g83v83xaguq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated API key configuration:\n",
      "OPENAI_API_KEY: Set correctly\n",
      "‚úÖ LLM reinitialized with correct API key\n"
     ]
    }
   ],
   "source": [
    "# Test entity extraction with the correct API key\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Reload environment variables\n",
    "load_dotenv(Path(\"..\") / \".env\", override=True)\n",
    "\n",
    "print(\"Updated API key configuration:\")\n",
    "print(f\"OPENAI_API_KEY: {'Set correctly' if os.getenv('OPENAI_API_KEY', '').startswith('sk-svcacct-') else 'Still using placeholder'}\")\n",
    "\n",
    "# Reinitialize LLM with correct API key\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "llm = ChatOpenAI(\n",
    "    model=os.getenv('LLM_MODEL', 'gpt-4-turbo'),\n",
    "    temperature=float(os.getenv('LLM_TEMPERATURE', '0.1')),\n",
    "    max_tokens=int(os.getenv('LLM_MAX_TOKENS', '2000')),\n",
    "    openai_api_key=os.getenv('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LLM reinitialized with correct API key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "rdqcl42h97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing entity extraction...\n",
      "Raw LLM response: '```json\\n{\\n  \"entities\": [\\n    {\\n      \"name\": \"OpenAI\",\\n      \"type\": \"Organization\",\\n      \"aliases\": [],\\n      \"confidence\": 0.95,\\n      \"context\": \"OpenAI announced GPT-4 at their conference in San'...\n",
      "Extracted JSON text: '{\\n  \"entities\": [\\n    {\\n      \"name\": \"OpenAI\",\\n      \"type\": \"Organization\",\\n      \"aliases\": [],\\n      \"confidence\": 0.95,\\n      \"context\": \"OpenAI announced GPT-4 at their conference in San Francis'...\n",
      "‚úÖ Successfully extracted 4 entities\n",
      "‚úÖ Extracted 4 entities:\n",
      "  1. OpenAI (Organization) - Confidence: 0.95\n",
      "  2. GPT-4 (Product) - Confidence: 0.95\n",
      "  3. San Francisco (Location) - Confidence: 0.95\n",
      "  4. Sam Altman (Person) - Confidence: 0.95\n"
     ]
    }
   ],
   "source": [
    "# Now test the entity extraction with a simple example\n",
    "import json\n",
    "import re\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Define Entity model\n",
    "class Entity(BaseModel):\n",
    "    name: str\n",
    "    type: str\n",
    "    aliases: List[str] = Field(default_factory=list)\n",
    "    confidence: float = Field(ge=0.0, le=1.0)\n",
    "    context: Optional[str] = None\n",
    "    properties: Dict[str, Any] = Field(default_factory=dict)\n",
    "\n",
    "# Entity extraction prompt\n",
    "ENTITY_EXTRACTION_PROMPT = \"\"\"\n",
    "You are an expert at extracting structured information from newsletter content. \n",
    "Extract entities from the following newsletter text and classify them into these categories:\n",
    "\n",
    "**Entity Types:**\n",
    "- **Organization**: Companies, institutions, government bodies\n",
    "- **Person**: Individuals mentioned in content\n",
    "- **Product**: Software, hardware, services, models\n",
    "- **Event**: Conferences, announcements, launches\n",
    "- **Location**: Geographic locations (cities, countries, regions)\n",
    "- **Topic**: Subject areas, technologies, fields of study\n",
    "\n",
    "**Instructions:**\n",
    "1. Extract entities with high confidence (>0.7)\n",
    "2. Provide alternative names/aliases if mentioned\n",
    "3. Include context where the entity was mentioned\n",
    "4. Rate confidence from 0.0 to 1.0\n",
    "5. Return results as valid JSON\n",
    "\n",
    "**Newsletter Content:**\n",
    "{content}\n",
    "\n",
    "**Required JSON Format:**\n",
    "```json\n",
    "{{\n",
    "  \"entities\": [\n",
    "    {{\n",
    "      \"name\": \"Entity Name\",\n",
    "      \"type\": \"Organization|Person|Product|Event|Location|Topic\",\n",
    "      \"aliases\": [\"Alternative Name 1\", \"Alternative Name 2\"],\n",
    "      \"confidence\": 0.95,\n",
    "      \"context\": \"The sentence or phrase where this entity was mentioned\",\n",
    "      \"properties\": {{\n",
    "        \"additional_info\": \"any relevant details\"\n",
    "      }}\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "```\n",
    "\n",
    "Return only valid JSON, no additional text.\n",
    "\"\"\"\n",
    "\n",
    "def extract_entities_with_llm_fixed(content: str) -> List[Entity]:\n",
    "    \"\"\"Extract entities from content using LLM with improved JSON parsing.\"\"\"\n",
    "    try:\n",
    "        # Create prompt\n",
    "        prompt = ENTITY_EXTRACTION_PROMPT.format(content=content)\n",
    "        \n",
    "        # Get LLM response\n",
    "        response = llm.invoke(prompt)\n",
    "        \n",
    "        print(f\"Raw LLM response: {repr(response.content[:200])}...\")\n",
    "        \n",
    "        # Try to extract JSON from the response\n",
    "        response_text = response.content.strip()\n",
    "        \n",
    "        # Look for JSON block markers and extract content\n",
    "        if '```json' in response_text:\n",
    "            # Extract content between ```json and ```\n",
    "            json_match = re.search(r'```json\\s*(.*?)\\s*```', response_text, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_text = json_match.group(1).strip()\n",
    "            else:\n",
    "                json_text = response_text\n",
    "        elif '```' in response_text:\n",
    "            # Extract content between any ``` blocks\n",
    "            json_match = re.search(r'```\\s*(.*?)\\s*```', response_text, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_text = json_match.group(1).strip()\n",
    "            else:\n",
    "                json_text = response_text\n",
    "        else:\n",
    "            json_text = response_text\n",
    "        \n",
    "        print(f\"Extracted JSON text: {repr(json_text[:200])}...\")\n",
    "        \n",
    "        # Parse JSON response\n",
    "        result = json.loads(json_text)\n",
    "        \n",
    "        # Convert to Entity objects\n",
    "        entities = []\n",
    "        for entity_data in result.get('entities', []):\n",
    "            entity = Entity(\n",
    "                name=entity_data['name'],\n",
    "                type=entity_data['type'],\n",
    "                aliases=entity_data.get('aliases', []),\n",
    "                confidence=entity_data['confidence'],\n",
    "                context=entity_data.get('context'),\n",
    "                properties=entity_data.get('properties', {})\n",
    "            )\n",
    "            entities.append(entity)\n",
    "        \n",
    "        print(f\"‚úÖ Successfully extracted {len(entities)} entities\")\n",
    "        return entities\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting entities: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test with simple content\n",
    "test_content = \"OpenAI announced GPT-4 at their conference in San Francisco. CEO Sam Altman presented the new AI model.\"\n",
    "\n",
    "print(\"üîç Testing entity extraction...\")\n",
    "extracted_entities = extract_entities_with_llm_fixed(test_content)\n",
    "\n",
    "if extracted_entities:\n",
    "    print(f\"‚úÖ Extracted {len(extracted_entities)} entities:\")\n",
    "    for i, entity in enumerate(extracted_entities):\n",
    "        print(f\"  {i+1}. {entity.name} ({entity.type}) - Confidence: {entity.confidence}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No entities extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "zgtpifmdxia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text (first 200 chars): # AI Weekly Newsletter #245 Welcome to this week's AI updates! ## üöÄ Major Announcements **OpenAI** announced the release of **GPT-5** at their developer conference in **San Francisco**. CEO **Sam Altm...\n",
      "\n",
      "üîç Testing entity extraction with newsletter content...\n",
      "Raw LLM response: '```json\\n{\\n  \"entities\": [\\n    {\\n      \"name\": \"OpenAI\",\\n      \"type\": \"Organization\",\\n      \"aliases\": [],\\n      \"confidence\": 0.95,\\n      \"context\": \"OpenAI announced the release of GPT-5 at their de'...\n",
      "Extracted JSON text: '{\\n  \"entities\": [\\n    {\\n      \"name\": \"OpenAI\",\\n      \"type\": \"Organization\",\\n      \"aliases\": [],\\n      \"confidence\": 0.95,\\n      \"context\": \"OpenAI announced the release of GPT-5 at their developer '...\n",
      "‚úÖ Successfully extracted 18 entities\n",
      "‚úÖ Extracted 18 entities from newsletter:\n",
      "  1. OpenAI (Organization) - Confidence: 0.95\n",
      "  2. GPT-5 (Product) - Confidence: 0.95\n",
      "  3. San Francisco (Location) - Confidence: 0.95\n",
      "  4. Sam Altman (Person) - Confidence: 0.95\n",
      "  5. OpenAI DevDay 2024 (Event) - Confidence: 0.95\n",
      "  6. Microsoft (Organization) - Confidence: 0.95\n",
      "  7. Azure AI (Product) - Confidence: 0.95\n",
      "  8. Satya Nadella (Person) - Confidence: 0.95\n",
      "  9. Microsoft Build 2024 (Event) - Confidence: 0.95\n",
      "  10. Google (Organization) - Confidence: 0.95\n"
     ]
    }
   ],
   "source": [
    "# Now let's test with the actual newsletter content from the notebook\n",
    "\n",
    "# Sample newsletter HTML for testing (from the notebook)\n",
    "sample_newsletter_html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>AI Weekly Newsletter #245</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>AI Weekly Newsletter #245</h1>\n",
    "    <p>Welcome to this week's AI updates!</p>\n",
    "    \n",
    "    <h2>üöÄ Major Announcements</h2>\n",
    "    <p><strong>OpenAI</strong> announced the release of <strong>GPT-5</strong> at their developer conference in <strong>San Francisco</strong>. \n",
    "    CEO <strong>Sam Altman</strong> presented the new capabilities during the <strong>OpenAI DevDay 2024</strong> event.</p>\n",
    "    \n",
    "    <h2>üè¢ Company Updates</h2>\n",
    "    <p><strong>Microsoft</strong> expanded their <strong>Azure AI</strong> services with new enterprise features. \n",
    "    The announcement was made by <strong>Satya Nadella</strong> during the <strong>Microsoft Build 2024</strong> conference.</p>\n",
    "    \n",
    "    <h2>üéØ Industry News</h2>\n",
    "    <p><strong>Google</strong> launched their new <strong>Gemini Pro</strong> model, focusing on <strong>AI Safety</strong> and \n",
    "    <strong>Responsible AI</strong> development. The launch event was held at <strong>Google I/O 2024</strong> in <strong>Mountain View</strong>.</p>\n",
    "    \n",
    "    <h2>üéì Educational Content</h2>\n",
    "    <p><strong>Stanford University</strong> announced a new <strong>AI Safety</strong> course taught by renowned professor \n",
    "    <strong>Fei-Fei Li</strong>. The course will cover <strong>Machine Learning</strong> ethics and <strong>AI Alignment</strong>.</p>\n",
    "    \n",
    "    <h2>üí° Research Highlights</h2>\n",
    "    <p>New research on <strong>Quantum Computing</strong> applications in <strong>AI</strong> was published by researchers at \n",
    "    <strong>MIT</strong> and <strong>IBM Research</strong>. The paper explores <strong>Quantum Machine Learning</strong> algorithms.</p>\n",
    "    \n",
    "    <p>Thanks for reading! See you next week.</p>\n",
    "    <p>Best regards,<br>The AI Weekly Team</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Clean the HTML content (using the function from the notebook)\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "\n",
    "def clean_html_content(html_content: str) -> str:\n",
    "    \"\"\"Clean and extract text from HTML content.\"\"\"\n",
    "    try:\n",
    "        # Parse HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Convert to text using html2text for better formatting\n",
    "        h = html2text.HTML2Text()\n",
    "        h.ignore_links = False\n",
    "        h.ignore_images = True\n",
    "        h.body_width = 0  # Don't wrap lines\n",
    "        \n",
    "        # Get text content\n",
    "        text_content = h.handle(str(soup))\n",
    "        \n",
    "        # Clean up extra whitespace\n",
    "        cleaned_text = ' '.join(text_content.split())\n",
    "        \n",
    "        return cleaned_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cleaning HTML: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "cleaned_text = clean_html_content(sample_newsletter_html)\n",
    "print(f\"Cleaned text (first 200 chars): {cleaned_text[:200]}...\")\n",
    "\n",
    "# Test entity extraction with newsletter content\n",
    "print(\"\\nüîç Testing entity extraction with newsletter content...\")\n",
    "newsletter_entities = extract_entities_with_llm_fixed(cleaned_text[:1000])  # First 1000 chars\n",
    "\n",
    "if newsletter_entities:\n",
    "    print(f\"‚úÖ Extracted {len(newsletter_entities)} entities from newsletter:\")\n",
    "    for i, entity in enumerate(newsletter_entities[:10]):  # Show first 10\n",
    "        print(f\"  {i+1}. {entity.name} ({entity.type}) - Confidence: {entity.confidence}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No entities extracted from newsletter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "tfzu69ltrj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker containers status:\n",
      "CONTAINER ID   IMAGE                              COMMAND                  CREATED          STATUS          PORTS                                                                                      NAMES\n",
      "4401d83263cd   neo4j:5.15.0                       \"tini -g -- /startup‚Ä¶\"   18 minutes ago   Up 18 minutes   0.0.0.0:7474->7474/tcp, [::]:7474->7474/tcp, 0.0.0.0:7687->7687/tcp, [::]:7687->7687/tcp   neo4j-dev\n",
      "2132b489da31   ghcr.io/github/github-mcp-server   \"/server/github-mcp-‚Ä¶\"   23 hours ago     Up 23 hours                                                                                                nostalgic_snyder\n",
      "688565dee75e   n8nio/n8n:latest                   \"tini -- /docker-ent‚Ä¶\"   38 hours ago     Up 38 hours     0.0.0.0:5678->5678/tcp, [::]:5678->5678/tcp                                                arrgh-n8n-n8n-1\n",
      "c27f8e9b8910   postgres:14                        \"docker-entrypoint.s‚Ä¶\"   38 hours ago     Up 38 hours     0.0.0.0:5432->5432/tcp, [::]:5432->5432/tcp                                                arrgh-n8n-postgres-1\n",
      "b4bfee2fc33b   bc972fc22f00                       \"node /app/dist/inde‚Ä¶\"   2 weeks ago      Up 2 weeks                                                                                                 hungry_cartwright\n",
      "f2a7785d91ef   bc972fc22f00                       \"node /app/dist/inde‚Ä¶\"   2 weeks ago      Up 2 weeks                                                                                                 great_lamport\n",
      "469dc070ad36   bc972fc22f00                       \"node /app/dist/inde‚Ä¶\"   2 weeks ago      Up 2 weeks                                                                                                 great_jepsen\n",
      "1b798bad3da3   genai                              \"uvicorn src.main:ap‚Ä¶\"   2 weeks ago      Up 2 weeks      0.0.0.0:8080->8080/tcp, [::]:8080->8080/tcp                                                genai\n",
      "c9b76b854fe9   mcp/github                         \"node dist/index.js ‚Ä¶\"   2 weeks ago      Up 2 weeks                                                                                                 eloquent_brattain\n",
      "\n",
      "‚úÖ Neo4j container is running\n"
     ]
    }
   ],
   "source": [
    "# Let's test if the notebook actually creates entries in Neo4j\n",
    "# First, let's check if Neo4j is still running\n",
    "import subprocess\n",
    "result = subprocess.run(['docker', 'ps'], capture_output=True, text=True)\n",
    "print(\"Docker containers status:\")\n",
    "print(result.stdout)\n",
    "\n",
    "# Check if neo4j-dev container is running\n",
    "if 'neo4j-dev' in result.stdout:\n",
    "    print(\"‚úÖ Neo4j container is running\")\n",
    "else:\n",
    "    print(\"‚ùå Neo4j container is not running\")\n",
    "    print(\"Starting Neo4j...\")\n",
    "    subprocess.run(['./scripts/start-neo4j.sh'], cwd='/Users/paulbonneville/Developer/arrgh-fastapi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aw7w7zpthrh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Neo4j connection established\n",
      "Connection successful: True\n",
      "\n",
      "Current database contents: [{'labels': ['Newsletter'], 'count': 1}]\n",
      "Total nodes in database: 1\n",
      "üîå Neo4j connection closed\n"
     ]
    }
   ],
   "source": [
    "# Now let's test the complete pipeline and see if it actually creates entries in Neo4j\n",
    "from neo4j import GraphDatabase\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Load environment\n",
    "load_dotenv(Path(\"..\") / \".env\", override=True)\n",
    "\n",
    "# Re-create the classes from the notebook\n",
    "class Neo4jConnection:\n",
    "    def __init__(self, uri: str, user: str, password: str):\n",
    "        self.uri = uri\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.driver = None\n",
    "        \n",
    "    def connect(self):\n",
    "        \"\"\"Establish connection to Neo4j.\"\"\"\n",
    "        try:\n",
    "            self.driver = GraphDatabase.driver(\n",
    "                self.uri, \n",
    "                auth=(self.user, self.password)\n",
    "            )\n",
    "            # Test connection\n",
    "            with self.driver.session() as session:\n",
    "                result = session.run(\"RETURN 1 as test\")\n",
    "                result.single()\n",
    "            print(\"‚úÖ Neo4j connection established\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Neo4j connection failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close Neo4j connection.\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.close()\n",
    "            print(\"üîå Neo4j connection closed\")\n",
    "    \n",
    "    def execute_query(self, query: str, parameters: dict = None):\n",
    "        \"\"\"Execute a Cypher query.\"\"\"\n",
    "        if not self.driver:\n",
    "            print(\"‚ùå No Neo4j connection\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            with self.driver.session() as session:\n",
    "                result = session.run(query, parameters or {})\n",
    "                return result.data()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Query execution failed: {e}\")\n",
    "            return None\n",
    "\n",
    "# Initialize Neo4j connection\n",
    "neo4j_conn = Neo4jConnection(\n",
    "    uri=os.getenv('NEO4J_URI', 'bolt://localhost:7687'),\n",
    "    user=os.getenv('NEO4J_USER', 'neo4j'),\n",
    "    password=os.getenv('NEO4J_PASSWORD')\n",
    ")\n",
    "\n",
    "# Test connection\n",
    "connection_success = neo4j_conn.connect()\n",
    "print(f\"Connection successful: {connection_success}\")\n",
    "\n",
    "if connection_success:\n",
    "    # Check what's currently in the database\n",
    "    current_data = neo4j_conn.execute_query(\"MATCH (n) RETURN labels(n) as labels, count(n) as count\")\n",
    "    print(f\"\\nCurrent database contents: {current_data}\")\n",
    "    \n",
    "    # Get total node count\n",
    "    total_nodes = neo4j_conn.execute_query(\"MATCH (n) RETURN count(n) as total\")\n",
    "    print(f\"Total nodes in database: {total_nodes[0]['total'] if total_nodes else 0}\")\n",
    "    \n",
    "    neo4j_conn.close()\n",
    "else:\n",
    "    print(\"Cannot proceed without Neo4j connection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "gmdecrr9hc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sample newsletter for testing\n",
      "Subject: AI Weekly Newsletter #245 - Test\n",
      "Newsletter ID: ai-weekly-245-test\n"
     ]
    }
   ],
   "source": [
    "# Let's run the complete pipeline from the notebook to see if it adds entries\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Define the models\n",
    "class Entity(BaseModel):\n",
    "    name: str\n",
    "    type: str\n",
    "    aliases: List[str] = Field(default_factory=list)\n",
    "    confidence: float = Field(ge=0.0, le=1.0)\n",
    "    context: Optional[str] = None\n",
    "    properties: Dict[str, Any] = Field(default_factory=dict)\n",
    "\n",
    "class Newsletter(BaseModel):\n",
    "    html_content: str\n",
    "    subject: str\n",
    "    sender: str\n",
    "    received_date: Optional[datetime] = None\n",
    "    newsletter_id: Optional[str] = None\n",
    "\n",
    "# GraphOperations class from the notebook\n",
    "class GraphOperations:\n",
    "    def __init__(self, neo4j_connection):\n",
    "        self.neo4j_conn = neo4j_connection\n",
    "    \n",
    "    def create_or_update_entity(self, entity: Entity) -> dict:\n",
    "        \"\"\"Create or update an entity node in the graph.\"\"\"\n",
    "        query = f\"\"\"\n",
    "        MERGE (e:{entity.type} {{name: $name}})\n",
    "        ON CREATE SET \n",
    "            e.created_at = datetime(),\n",
    "            e.confidence = $confidence,\n",
    "            e.aliases = $aliases,\n",
    "            e.mention_count = 1,\n",
    "            e.properties = $properties\n",
    "        ON MATCH SET\n",
    "            e.last_seen = datetime(),\n",
    "            e.mention_count = e.mention_count + 1,\n",
    "            e.confidence = CASE \n",
    "                WHEN $confidence > e.confidence THEN $confidence \n",
    "                ELSE e.confidence \n",
    "            END\n",
    "        RETURN e, \n",
    "               CASE WHEN e.created_at = e.last_seen THEN 'created' ELSE 'updated' END as operation\n",
    "        \"\"\"\n",
    "        \n",
    "        parameters = {\n",
    "            'name': entity.name,\n",
    "            'confidence': entity.confidence,\n",
    "            'aliases': entity.aliases,\n",
    "            'properties': entity.properties\n",
    "        }\n",
    "        \n",
    "        result = self.neo4j_conn.execute_query(query, parameters)\n",
    "        return result[0] if result else None\n",
    "    \n",
    "    def create_newsletter_node(self, newsletter: Newsletter) -> dict:\n",
    "        \"\"\"Create a newsletter node in the graph.\"\"\"\n",
    "        query = \"\"\"\n",
    "        CREATE (n:Newsletter {\n",
    "            id: $newsletter_id,\n",
    "            subject: $subject,\n",
    "            sender: $sender,\n",
    "            received_date: $received_date,\n",
    "            created_at: datetime(),\n",
    "            content_length: $content_length\n",
    "        })\n",
    "        RETURN n\n",
    "        \"\"\"\n",
    "        \n",
    "        parameters = {\n",
    "            'newsletter_id': newsletter.newsletter_id,\n",
    "            'subject': newsletter.subject,\n",
    "            'sender': newsletter.sender,\n",
    "            'received_date': newsletter.received_date,\n",
    "            'content_length': len(newsletter.html_content)\n",
    "        }\n",
    "        \n",
    "        result = self.neo4j_conn.execute_query(query, parameters)\n",
    "        return result[0] if result else None\n",
    "    \n",
    "    def link_entity_to_newsletter(self, entity_name: str, entity_type: str, newsletter_id: str, context: str = None) -> dict:\n",
    "        \"\"\"Create a MENTIONED_IN relationship between entity and newsletter.\"\"\"\n",
    "        query = f\"\"\"\n",
    "        MATCH (e:{entity_type} {{name: $entity_name}})\n",
    "        MATCH (n:Newsletter {{id: $newsletter_id}})\n",
    "        CREATE (e)-[r:MENTIONED_IN {{\n",
    "            date: datetime(),\n",
    "            context: $context\n",
    "        }}]->(n)\n",
    "        RETURN r\n",
    "        \"\"\"\n",
    "        \n",
    "        parameters = {\n",
    "            'entity_name': entity_name,\n",
    "            'newsletter_id': newsletter_id,\n",
    "            'context': context\n",
    "        }\n",
    "        \n",
    "        result = self.neo4j_conn.execute_query(query, parameters)\n",
    "        return result[0] if result else None\n",
    "\n",
    "# Create sample newsletter\n",
    "sample_newsletter = Newsletter(\n",
    "    html_content=sample_newsletter_html,\n",
    "    subject=\"AI Weekly Newsletter #245 - Test\",\n",
    "    sender=\"ai-weekly@example.com\",\n",
    "    received_date=datetime.now(timezone.utc),\n",
    "    newsletter_id=\"ai-weekly-245-test\"\n",
    ")\n",
    "\n",
    "print(\"Created sample newsletter for testing\")\n",
    "print(f\"Subject: {sample_newsletter.subject}\")\n",
    "print(f\"Newsletter ID: {sample_newsletter.newsletter_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1yawk9xkwbk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Neo4j connection established\n",
      "=== INITIAL STATE ===\n",
      "Initial graph statistics:\n",
      "  Organizations: 0\n",
      "  People: 0\n",
      "  Products: 0\n",
      "  Events: 0\n",
      "  Locations: 0\n",
      "  Topics: 0\n",
      "  Newsletters: 1\n",
      "  Relationships: 0\n",
      "\n",
      "=== STEP 1: HTML CLEANING ===\n",
      "‚úÖ Cleaned text length: 1160 characters\n",
      "\n",
      "=== STEP 2: ENTITY EXTRACTION ===\n",
      "Raw LLM response: '```json\\n{\\n  \"entities\": [\\n    {\\n      \"name\": \"OpenAI\",\\n      \"type\": \"Organization\",\\n      \"aliases\": [],\\n      \"confidence\": 0.98,\\n      \"context\": \"OpenAI announced the release of GPT-5 at their de'...\n",
      "Extracted JSON text: '{\\n  \"entities\": [\\n    {\\n      \"name\": \"OpenAI\",\\n      \"type\": \"Organization\",\\n      \"aliases\": [],\\n      \"confidence\": 0.98,\\n      \"context\": \"OpenAI announced the release of GPT-5 at their developer '...\n",
      "‚úÖ Successfully extracted 20 entities\n",
      "‚úÖ Extracted 20 entities\n",
      "\n",
      "=== STEP 3: CREATE NEWSLETTER NODE ===\n",
      "‚úÖ Newsletter node created\n",
      "\n",
      "=== STEP 4: PROCESS ENTITIES ===\n",
      "Processing entity 1/20: OpenAI (Organization)\n",
      "‚ùå Query execution failed: {code: Neo.ClientError.Statement.TypeError} {message: Property values can only be of primitive types or arrays thereof. Encountered: Map{}.}\n",
      "  ‚ùå Failed to process entity: OpenAI\n",
      "Processing entity 2/20: GPT-5 (Product)\n",
      "‚ùå Query execution failed: {code: Neo.ClientError.Statement.TypeError} {message: Property values can only be of primitive types or arrays thereof. Encountered: Map{}.}\n",
      "  ‚ùå Failed to process entity: GPT-5\n",
      "Processing entity 3/20: San Francisco (Location)\n",
      "‚ùå Query execution failed: {code: Neo.ClientError.Statement.TypeError} {message: Property values can only be of primitive types or arrays thereof. Encountered: Map{}.}\n",
      "  ‚ùå Failed to process entity: San Francisco\n",
      "Processing entity 4/20: Sam Altman (Person)\n",
      "‚ùå Query execution failed: {code: Neo.ClientError.Statement.TypeError} {message: Property values can only be of primitive types or arrays thereof. Encountered: Map{}.}\n",
      "  ‚ùå Failed to process entity: Sam Altman\n",
      "Processing entity 5/20: OpenAI DevDay 2024 (Event)\n",
      "‚ùå Query execution failed: {code: Neo.ClientError.Statement.TypeError} {message: Property values can only be of primitive types or arrays thereof. Encountered: Map{}.}\n",
      "  ‚ùå Failed to process entity: OpenAI DevDay 2024\n",
      "Processing entity 6/20: Microsoft (Organization)\n",
      "‚ùå Query execution failed: {code: Neo.ClientError.Statement.TypeError} {message: Property values can only be of primitive types or arrays thereof. Encountered: Map{}.}\n",
      "  ‚ùå Failed to process entity: Microsoft\n",
      "Processing entity 7/20: Azure AI (Product)\n",
      "‚ùå Query execution failed: {code: Neo.ClientError.Statement.TypeError} {message: Property values can only be of primitive types or arrays thereof. Encountered: Map{}.}\n",
      "  ‚ùå Failed to process entity: Azure AI\n",
      "Processing entity 8/20: Satya Nadella (Person)\n",
      "‚ùå Query execution failed: {code: Neo.ClientError.Statement.TypeError} {message: Property values can only be of primitive types or arrays thereof. Encountered: Map{}.}\n",
      "  ‚ùå Failed to process entity: Satya Nadella\n",
      "Processing entity 9/20: Microsoft Build 2024 (Event)\n",
      "‚ùå Query execution failed: {code: Neo.ClientError.Statement.TypeError} {message: Property values can only be of primitive types or arrays thereof. Encountered: Map{}.}\n",
      "  ‚ùå Failed to process entity: Microsoft Build 2024\n",
      "Processing entity 10/20: Google (Organization)\n",
      "‚ùå Query execution failed: {code: Neo.ClientError.Statement.TypeError} {message: Property values can only be of primitive types or arrays thereof. Encountered: Map{}.}\n",
      "  ‚ùå Failed to process entity: Google\n",
      "Processing entity 11/20: Gemini Pro (Product)\n",
      "‚ùå Query execution failed: {code: Neo.ClientError.Statement.TypeError} {message: Property values can only be of primitive types or arrays thereof. Encountered: Map{}.}\n",
      "  ‚ùå Failed to process entity: Gemini Pro\n",
      "Processing entity 12/20: AI Safety (Topic)\n",
      "‚ùå Query execution failed: {code: Neo.ClientError.Statement.TypeError} {message: Property values can only be of primitive types or arrays thereof. Encountered: Map{}.}\n",
      "  ‚ùå Failed to process entity: AI Safety\n",
      "Processing entity 13/20: Google I/O 2024 (Event)\n",
      "‚ùå Query execution failed: {code: Neo.ClientError.Statement.TypeError} {message: Property values can only be of primitive types or arrays thereof. Encountered: Map{}.}\n",
      "  ‚ùå Failed to process entity: Google I/O 2024\n",
      "Processing entity 14/20: Mountain View (Location)\n",
      "‚ùå Query execution failed: {code: Neo.ClientError.Statement.TypeError} {message: Property values can only be of primitive types or arrays thereof. Encountered: Map{}.}\n",
      "  ‚ùå Failed to process entity: Mountain View\n",
      "Processing entity 15/20: Stanford University (Organization)\n",
      "‚ùå Query execution failed: {code: Neo.ClientError.Statement.TypeError} {message: Property values can only be of primitive types or arrays thereof. Encountered: Map{}.}\n",
      "  ‚ùå Failed to process entity: Stanford University\n",
      "Processing entity 16/20: Fei-Fei Li (Person)\n",
      "‚ùå Query execution failed: {code: Neo.ClientError.Statement.TypeError} {message: Property values can only be of primitive types or arrays thereof. Encountered: Map{}.}\n",
      "  ‚ùå Failed to process entity: Fei-Fei Li\n",
      "Processing entity 17/20: Machine Learning (Topic)\n",
      "‚ùå Query execution failed: {code: Neo.ClientError.Statement.TypeError} {message: Property values can only be of primitive types or arrays thereof. Encountered: Map{}.}\n",
      "  ‚ùå Failed to process entity: Machine Learning\n",
      "Processing entity 18/20: MIT (Organization)\n",
      "‚ùå Query execution failed: {code: Neo.ClientError.Statement.TypeError} {message: Property values can only be of primitive types or arrays thereof. Encountered: Map{}.}\n",
      "  ‚ùå Failed to process entity: MIT\n",
      "Processing entity 19/20: IBM Research (Organization)\n",
      "‚ùå Query execution failed: {code: Neo.ClientError.Statement.TypeError} {message: Property values can only be of primitive types or arrays thereof. Encountered: Map{}.}\n",
      "  ‚ùå Failed to process entity: IBM Research\n",
      "Processing entity 20/20: Quantum Computing (Topic)\n",
      "‚ùå Query execution failed: {code: Neo.ClientError.Statement.TypeError} {message: Property values can only be of primitive types or arrays thereof. Encountered: Map{}.}\n",
      "  ‚ùå Failed to process entity: Quantum Computing\n",
      "\n",
      "üìä Processing Summary:\n",
      "  New entities created: 0\n",
      "  Existing entities updated: 0\n",
      "  Entity breakdown: {'Organization': 0, 'Person': 0, 'Product': 0, 'Event': 0, 'Location': 0, 'Topic': 0}\n"
     ]
    }
   ],
   "source": [
    "# Now let's run the complete pipeline and see what gets added to Neo4j\n",
    "import time\n",
    "\n",
    "# Connect to Neo4j\n",
    "neo4j_conn.connect()\n",
    "graph_ops = GraphOperations(neo4j_conn)\n",
    "\n",
    "# Get initial state\n",
    "print(\"=== INITIAL STATE ===\")\n",
    "initial_stats = neo4j_conn.execute_query(\"\"\"\n",
    "CALL {\n",
    "    MATCH (o:Organization) RETURN count(o) as organizations\n",
    "}\n",
    "CALL {\n",
    "    MATCH (p:Person) RETURN count(p) as people\n",
    "}\n",
    "CALL {\n",
    "    MATCH (pr:Product) RETURN count(pr) as products\n",
    "}\n",
    "CALL {\n",
    "    MATCH (e:Event) RETURN count(e) as events\n",
    "}\n",
    "CALL {\n",
    "    MATCH (l:Location) RETURN count(l) as locations\n",
    "}\n",
    "CALL {\n",
    "    MATCH (t:Topic) RETURN count(t) as topics\n",
    "}\n",
    "CALL {\n",
    "    MATCH (n:Newsletter) RETURN count(n) as newsletters\n",
    "}\n",
    "CALL {\n",
    "    MATCH ()-[r]->() RETURN count(r) as relationships\n",
    "}\n",
    "RETURN organizations, people, products, events, locations, topics, newsletters, relationships\n",
    "\"\"\")\n",
    "\n",
    "print(\"Initial graph statistics:\")\n",
    "if initial_stats:\n",
    "    for key, value in initial_stats[0].items():\n",
    "        print(f\"  {key.capitalize()}: {value}\")\n",
    "\n",
    "# Step 1: Clean HTML content\n",
    "print(\"\\n=== STEP 1: HTML CLEANING ===\")\n",
    "cleaned_text = clean_html_content(sample_newsletter.html_content)\n",
    "print(f\"‚úÖ Cleaned text length: {len(cleaned_text)} characters\")\n",
    "\n",
    "# Step 2: Extract entities\n",
    "print(\"\\n=== STEP 2: ENTITY EXTRACTION ===\")\n",
    "entities = extract_entities_with_llm_fixed(cleaned_text)\n",
    "print(f\"‚úÖ Extracted {len(entities)} entities\")\n",
    "\n",
    "# Step 3: Create newsletter node\n",
    "print(\"\\n=== STEP 3: CREATE NEWSLETTER NODE ===\")\n",
    "try:\n",
    "    newsletter_node = graph_ops.create_newsletter_node(sample_newsletter)\n",
    "    if newsletter_node:\n",
    "        print(\"‚úÖ Newsletter node created\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to create newsletter node\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating newsletter node: {e}\")\n",
    "\n",
    "# Step 4: Process entities and create nodes\n",
    "print(\"\\n=== STEP 4: PROCESS ENTITIES ===\")\n",
    "entity_summary = {'Organization': 0, 'Person': 0, 'Product': 0, 'Event': 0, 'Location': 0, 'Topic': 0}\n",
    "new_entities = 0\n",
    "updated_entities = 0\n",
    "\n",
    "for i, entity in enumerate(entities):\n",
    "    try:\n",
    "        print(f\"Processing entity {i+1}/{len(entities)}: {entity.name} ({entity.type})\")\n",
    "        \n",
    "        # Create or update entity\n",
    "        result = graph_ops.create_or_update_entity(entity)\n",
    "        if result:\n",
    "            operation = result.get('operation', 'unknown')\n",
    "            if operation == 'created':\n",
    "                new_entities += 1\n",
    "                print(f\"  ‚úÖ Created new entity: {entity.name}\")\n",
    "            else:\n",
    "                updated_entities += 1\n",
    "                print(f\"  ‚úÖ Updated existing entity: {entity.name}\")\n",
    "            \n",
    "            # Link to newsletter\n",
    "            link_result = graph_ops.link_entity_to_newsletter(\n",
    "                entity.name, \n",
    "                entity.type, \n",
    "                sample_newsletter.newsletter_id, \n",
    "                entity.context\n",
    "            )\n",
    "            if link_result:\n",
    "                print(f\"  ‚úÖ Linked to newsletter\")\n",
    "            \n",
    "            # Update summary\n",
    "            entity_summary[entity.type] += 1\n",
    "        else:\n",
    "            print(f\"  ‚ùå Failed to process entity: {entity.name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error processing entity {entity.name}: {e}\")\n",
    "\n",
    "print(f\"\\nüìä Processing Summary:\")\n",
    "print(f\"  New entities created: {new_entities}\")\n",
    "print(f\"  Existing entities updated: {updated_entities}\")\n",
    "print(f\"  Entity breakdown: {entity_summary}\")\n",
    "\n",
    "time.sleep(1)  # Give Neo4j a moment to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fpef4160y06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created fixed GraphOperations class\n",
      "Testing with entity: OpenAI (Organization)\n",
      "Properties: {}\n",
      "‚úÖ Successfully created/updated entity: OpenAI\n",
      "Operation: updated\n",
      "‚úÖ Successfully linked entity to newsletter\n",
      "üîå Neo4j connection closed\n"
     ]
    }
   ],
   "source": [
    "# Let's fix the entity creation query to handle the properties correctly\n",
    "class GraphOperationsFixed:\n",
    "    def __init__(self, neo4j_connection):\n",
    "        self.neo4j_conn = neo4j_connection\n",
    "    \n",
    "    def create_or_update_entity(self, entity: Entity) -> dict:\n",
    "        \"\"\"Create or update an entity node in the graph.\"\"\"\n",
    "        # Convert properties to a JSON string if it's not empty, otherwise set to null\n",
    "        properties_str = None\n",
    "        if entity.properties:\n",
    "            import json\n",
    "            properties_str = json.dumps(entity.properties)\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        MERGE (e:{entity.type} {{name: $name}})\n",
    "        ON CREATE SET \n",
    "            e.created_at = datetime(),\n",
    "            e.confidence = $confidence,\n",
    "            e.aliases = $aliases,\n",
    "            e.mention_count = 1,\n",
    "            e.properties_json = $properties_json\n",
    "        ON MATCH SET\n",
    "            e.last_seen = datetime(),\n",
    "            e.mention_count = e.mention_count + 1,\n",
    "            e.confidence = CASE \n",
    "                WHEN $confidence > e.confidence THEN $confidence \n",
    "                ELSE e.confidence \n",
    "            END\n",
    "        RETURN e, \n",
    "               CASE WHEN e.created_at = e.last_seen THEN 'created' ELSE 'updated' END as operation\n",
    "        \"\"\"\n",
    "        \n",
    "        parameters = {\n",
    "            'name': entity.name,\n",
    "            'confidence': entity.confidence,\n",
    "            'aliases': entity.aliases,\n",
    "            'properties_json': properties_str\n",
    "        }\n",
    "        \n",
    "        result = self.neo4j_conn.execute_query(query, parameters)\n",
    "        return result[0] if result else None\n",
    "    \n",
    "    def create_newsletter_node(self, newsletter: Newsletter) -> dict:\n",
    "        \"\"\"Create a newsletter node in the graph.\"\"\"\n",
    "        query = \"\"\"\n",
    "        MERGE (n:Newsletter {id: $newsletter_id})\n",
    "        ON CREATE SET\n",
    "            n.subject = $subject,\n",
    "            n.sender = $sender,\n",
    "            n.received_date = $received_date,\n",
    "            n.created_at = datetime(),\n",
    "            n.content_length = $content_length\n",
    "        RETURN n\n",
    "        \"\"\"\n",
    "        \n",
    "        parameters = {\n",
    "            'newsletter_id': newsletter.newsletter_id,\n",
    "            'subject': newsletter.subject,\n",
    "            'sender': newsletter.sender,\n",
    "            'received_date': newsletter.received_date,\n",
    "            'content_length': len(newsletter.html_content)\n",
    "        }\n",
    "        \n",
    "        result = self.neo4j_conn.execute_query(query, parameters)\n",
    "        return result[0] if result else None\n",
    "    \n",
    "    def link_entity_to_newsletter(self, entity_name: str, entity_type: str, newsletter_id: str, context: str = None) -> dict:\n",
    "        \"\"\"Create a MENTIONED_IN relationship between entity and newsletter.\"\"\"\n",
    "        query = f\"\"\"\n",
    "        MATCH (e:{entity_type} {{name: $entity_name}})\n",
    "        MATCH (n:Newsletter {{id: $newsletter_id}})\n",
    "        MERGE (e)-[r:MENTIONED_IN]->(n)\n",
    "        ON CREATE SET\n",
    "            r.date = datetime(),\n",
    "            r.context = $context\n",
    "        RETURN r\n",
    "        \"\"\"\n",
    "        \n",
    "        parameters = {\n",
    "            'entity_name': entity_name,\n",
    "            'newsletter_id': newsletter_id,\n",
    "            'context': context\n",
    "        }\n",
    "        \n",
    "        result = self.neo4j_conn.execute_query(query, parameters)\n",
    "        return result[0] if result else None\n",
    "\n",
    "# Create new graph operations with the fix\n",
    "graph_ops_fixed = GraphOperationsFixed(neo4j_conn)\n",
    "\n",
    "print(\"‚úÖ Created fixed GraphOperations class\")\n",
    "\n",
    "# Let's test with a single entity first\n",
    "test_entity = entities[0]  # OpenAI\n",
    "print(f\"Testing with entity: {test_entity.name} ({test_entity.type})\")\n",
    "print(f\"Properties: {test_entity.properties}\")\n",
    "\n",
    "try:\n",
    "    result = graph_ops_fixed.create_or_update_entity(test_entity)\n",
    "    if result:\n",
    "        print(f\"‚úÖ Successfully created/updated entity: {test_entity.name}\")\n",
    "        print(f\"Operation: {result.get('operation', 'unknown')}\")\n",
    "        \n",
    "        # Now try to link it to the newsletter\n",
    "        link_result = graph_ops_fixed.link_entity_to_newsletter(\n",
    "            test_entity.name,\n",
    "            test_entity.type,\n",
    "            sample_newsletter.newsletter_id,\n",
    "            test_entity.context\n",
    "        )\n",
    "        if link_result:\n",
    "            print(\"‚úÖ Successfully linked entity to newsletter\")\n",
    "        else:\n",
    "            print(\"‚ùå Failed to link entity to newsletter\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to create/update entity\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "neo4j_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ckxa6mz4tye",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Neo4j connection established\n",
      "=== RUNNING COMPLETE FIXED PIPELINE ===\n",
      "Before processing:\n",
      "  Organizations: 1\n",
      "  People: 0\n",
      "  Products: 0\n",
      "  Events: 0\n",
      "  Locations: 0\n",
      "  Topics: 0\n",
      "  Newsletters: 2\n",
      "  Relationships: 1\n",
      "\n",
      "Processing 20 entities...\n",
      "  ‚úÖ Updated: OpenAI (Organization)\n",
      "  ‚úÖ Updated: GPT-5 (Product)\n",
      "  ‚úÖ Updated: San Francisco (Location)\n",
      "  ‚úÖ Updated: Sam Altman (Person)\n",
      "  ‚úÖ Updated: OpenAI DevDay 2024 (Event)\n",
      "  ‚úÖ Updated: Microsoft (Organization)\n",
      "  ‚úÖ Updated: Azure AI (Product)\n",
      "  ‚úÖ Updated: Satya Nadella (Person)\n",
      "  ‚úÖ Updated: Microsoft Build 2024 (Event)\n",
      "  ‚úÖ Updated: Google (Organization)\n",
      "\n",
      "üìä Processing Complete:\n",
      "  New entities created: 0\n",
      "  Existing entities updated: 10\n",
      "  Entity breakdown: {'Organization': 3, 'Person': 2, 'Product': 2, 'Event': 2, 'Location': 1, 'Topic': 0}\n",
      "\n",
      "After processing:\n",
      "  Organizations: 3\n",
      "  People: 2\n",
      "  Products: 2\n",
      "  Events: 2\n",
      "  Locations: 1\n",
      "  Topics: 0\n",
      "  Newsletters: 2\n",
      "  Relationships: 10\n",
      "\n",
      "=== SAMPLE ENTITIES IN DATABASE ===\n",
      "  OpenAI (Organization) - Confidence: 0.98, Mentions: 2\n",
      "  GPT-5 (Product) - Confidence: 0.99, Mentions: 1\n",
      "  San Francisco (Location) - Confidence: 0.95, Mentions: 1\n",
      "  Sam Altman (Person) - Confidence: 0.95, Mentions: 1\n",
      "  OpenAI DevDay 2024 (Event) - Confidence: 0.95, Mentions: 1\n",
      "  Microsoft (Organization) - Confidence: 0.98, Mentions: 1\n",
      "  Azure AI (Product) - Confidence: 0.97, Mentions: 1\n",
      "  Satya Nadella (Person) - Confidence: 0.95, Mentions: 1\n",
      "  Microsoft Build 2024 (Event) - Confidence: 0.95, Mentions: 1\n",
      "  Google (Organization) - Confidence: 0.98, Mentions: 1\n",
      "üîå Neo4j connection closed\n"
     ]
    }
   ],
   "source": [
    "# Now let's run the complete pipeline with the fixed version\n",
    "neo4j_conn.connect()\n",
    "\n",
    "print(\"=== RUNNING COMPLETE FIXED PIPELINE ===\")\n",
    "\n",
    "# Get initial state\n",
    "initial_stats = neo4j_conn.execute_query(\"\"\"\n",
    "CALL {\n",
    "    MATCH (o:Organization) RETURN count(o) as organizations\n",
    "}\n",
    "CALL {\n",
    "    MATCH (p:Person) RETURN count(p) as people  \n",
    "}\n",
    "CALL {\n",
    "    MATCH (pr:Product) RETURN count(pr) as products\n",
    "}\n",
    "CALL {\n",
    "    MATCH (e:Event) RETURN count(e) as events\n",
    "}\n",
    "CALL {\n",
    "    MATCH (l:Location) RETURN count(l) as locations\n",
    "}\n",
    "CALL {\n",
    "    MATCH (t:Topic) RETURN count(t) as topics\n",
    "}\n",
    "CALL {\n",
    "    MATCH (n:Newsletter) RETURN count(n) as newsletters\n",
    "}\n",
    "CALL {\n",
    "    MATCH ()-[r]->() RETURN count(r) as relationships\n",
    "}\n",
    "RETURN organizations, people, products, events, locations, topics, newsletters, relationships\n",
    "\"\"\")\n",
    "\n",
    "print(\"Before processing:\")\n",
    "if initial_stats:\n",
    "    for key, value in initial_stats[0].items():\n",
    "        print(f\"  {key.capitalize()}: {value}\")\n",
    "\n",
    "# Process all entities\n",
    "entity_summary = {'Organization': 0, 'Person': 0, 'Product': 0, 'Event': 0, 'Location': 0, 'Topic': 0}\n",
    "new_entities = 0\n",
    "updated_entities = 0\n",
    "\n",
    "print(f\"\\nProcessing {len(entities)} entities...\")\n",
    "\n",
    "for i, entity in enumerate(entities[:10]):  # Process first 10 to avoid too much output\n",
    "    try:\n",
    "        # Create or update entity\n",
    "        result = graph_ops_fixed.create_or_update_entity(entity)\n",
    "        if result:\n",
    "            operation = result.get('operation', 'unknown')\n",
    "            if operation == 'created':\n",
    "                new_entities += 1\n",
    "                print(f\"  ‚úÖ Created: {entity.name} ({entity.type})\")\n",
    "            else:\n",
    "                updated_entities += 1\n",
    "                print(f\"  ‚úÖ Updated: {entity.name} ({entity.type})\")\n",
    "            \n",
    "            # Link to newsletter\n",
    "            graph_ops_fixed.link_entity_to_newsletter(\n",
    "                entity.name, \n",
    "                entity.type, \n",
    "                sample_newsletter.newsletter_id, \n",
    "                entity.context\n",
    "            )\n",
    "            \n",
    "            # Update summary\n",
    "            entity_summary[entity.type] += 1\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error processing {entity.name}: {e}\")\n",
    "\n",
    "print(f\"\\nüìä Processing Complete:\")\n",
    "print(f\"  New entities created: {new_entities}\")\n",
    "print(f\"  Existing entities updated: {updated_entities}\")\n",
    "print(f\"  Entity breakdown: {entity_summary}\")\n",
    "\n",
    "# Get final state\n",
    "final_stats = neo4j_conn.execute_query(\"\"\"\n",
    "CALL {\n",
    "    MATCH (o:Organization) RETURN count(o) as organizations\n",
    "}\n",
    "CALL {\n",
    "    MATCH (p:Person) RETURN count(p) as people\n",
    "}\n",
    "CALL {\n",
    "    MATCH (pr:Product) RETURN count(pr) as products\n",
    "}\n",
    "CALL {\n",
    "    MATCH (e:Event) RETURN count(e) as events\n",
    "}\n",
    "CALL {\n",
    "    MATCH (l:Location) RETURN count(l) as locations\n",
    "}\n",
    "CALL {\n",
    "    MATCH (t:Topic) RETURN count(t) as topics\n",
    "}\n",
    "CALL {\n",
    "    MATCH (n:Newsletter) RETURN count(n) as newsletters\n",
    "}\n",
    "CALL {\n",
    "    MATCH ()-[r]->() RETURN count(r) as relationships\n",
    "}\n",
    "RETURN organizations, people, products, events, locations, topics, newsletters, relationships\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nAfter processing:\")\n",
    "if final_stats:\n",
    "    for key, value in final_stats[0].items():\n",
    "        print(f\"  {key.capitalize()}: {value}\")\n",
    "\n",
    "# Show some actual entities that were created\n",
    "print(\"\\n=== SAMPLE ENTITIES IN DATABASE ===\")\n",
    "sample_entities = neo4j_conn.execute_query(\"\"\"\n",
    "MATCH (n)\n",
    "WHERE NOT n:Newsletter\n",
    "RETURN labels(n)[0] as type, n.name as name, n.confidence as confidence, n.mention_count as mentions\n",
    "ORDER BY n.mention_count DESC\n",
    "LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "if sample_entities:\n",
    "    for entity in sample_entities:\n",
    "        print(f\"  {entity['name']} ({entity['type']}) - Confidence: {entity['confidence']}, Mentions: {entity['mentions']}\")\n",
    "\n",
    "neo4j_conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
