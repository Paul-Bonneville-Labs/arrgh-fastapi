{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newsletter Processing Pipeline Development\n",
    "\n",
    "This notebook contains the development and testing of the Arrgh! Aggregated Research Repository newsletter processing pipeline.\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "1. **Configuration Setup**: Load environment variables and settings\n",
    "2. **HTML Processing**: Parse and clean newsletter HTML content\n",
    "3. **Entity Extraction**: Use LLM to extract entities (6 types)\n",
    "4. **Graph Operations**: Connect to Neo4j and manage graph database\n",
    "5. **Entity Resolution**: Match extracted entities to existing nodes\n",
    "6. **Fact Extraction**: Extract relationships and temporal facts\n",
    "7. **Graph Updates**: Update Neo4j with new nodes and relationships\n",
    "8. **Summary Generation**: Create human-readable summaries\n",
    "\n",
    "## Entity Types\n",
    "- **Organization**: Companies, institutions, government bodies\n",
    "- **Person**: Individuals mentioned in content\n",
    "- **Product**: Software, hardware, services\n",
    "- **Event**: Conferences, announcements, launches\n",
    "- **Location**: Geographic locations\n",
    "- **Topic**: Subject areas, technologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, TypedDict\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "\n",
    "# Add src directory to path for imports\n",
    "src_path = Path(\"..\") / \"src\"\n",
    "sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(Path(\"..\") / \".env\")\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "\n",
    "# LLM and AI\n",
    "from openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "\n",
    "# Graph database\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration from environment variables\n",
    "class Config:\n",
    "    # LLM Configuration\n",
    "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "    LLM_MODEL = os.getenv(\"LLM_MODEL\", \"gpt-4-turbo\")\n",
    "    LLM_TEMPERATURE = float(os.getenv(\"LLM_TEMPERATURE\", \"0.1\"))\n",
    "    LLM_MAX_TOKENS = int(os.getenv(\"LLM_MAX_TOKENS\", \"2000\"))\n",
    "    \n",
    "    # Neo4j Configuration\n",
    "    NEO4J_URI = os.getenv(\"NEO4J_URI\", \"bolt://localhost:7687\")\n",
    "    NEO4J_USER = os.getenv(\"NEO4J_USER\", \"neo4j\")\n",
    "    NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "    NEO4J_DATABASE = os.getenv(\"NEO4J_DATABASE\", \"neo4j\")\n",
    "    \n",
    "    # Processing Configuration\n",
    "    MAX_ENTITIES_PER_NEWSLETTER = int(os.getenv(\"MAX_ENTITIES_PER_NEWSLETTER\", \"100\"))\n",
    "    FACT_EXTRACTION_BATCH_SIZE = int(os.getenv(\"FACT_EXTRACTION_BATCH_SIZE\", \"10\"))\n",
    "    PROCESSING_TIMEOUT = int(os.getenv(\"PROCESSING_TIMEOUT\", \"300\"))\n",
    "    ENTITY_CONFIDENCE_THRESHOLD = float(os.getenv(\"ENTITY_CONFIDENCE_THRESHOLD\", \"0.7\"))\n",
    "    FACT_CONFIDENCE_THRESHOLD = float(os.getenv(\"FACT_CONFIDENCE_THRESHOLD\", \"0.8\"))\n",
    "    \n",
    "    # Feature Flags\n",
    "    ENABLE_DEBUG_MODE = os.getenv(\"ENABLE_DEBUG_MODE\", \"false\").lower() == \"true\"\n",
    "    \n",
    "config = Config()\n",
    "\n",
    "print(\"üîß Configuration loaded:\")\n",
    "print(f\"  LLM Model: {config.LLM_MODEL}\")\n",
    "print(f\"  Neo4j URI: {config.NEO4J_URI}\")\n",
    "print(f\"  Max Entities: {config.MAX_ENTITIES_PER_NEWSLETTER}\")\n",
    "print(f\"  Debug Mode: {config.ENABLE_DEBUG_MODE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Models and Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data models for the processing pipeline\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "class Entity(BaseModel):\n",
    "    \"\"\"Represents an extracted entity from newsletter content.\"\"\"\n",
    "    name: str\n",
    "    type: str  # Organization, Person, Product, Event, Location, Topic\n",
    "    aliases: List[str] = Field(default_factory=list)\n",
    "    confidence: float = Field(ge=0.0, le=1.0)\n",
    "    context: Optional[str] = None\n",
    "    properties: Dict[str, Any] = Field(default_factory=dict)\n",
    "\n",
    "class Fact(BaseModel):\n",
    "    \"\"\"Represents a relationship or fact between entities.\"\"\"\n",
    "    subject_entity: str\n",
    "    predicate: str  # relationship type\n",
    "    object_entity: str\n",
    "    confidence: float = Field(ge=0.0, le=1.0)\n",
    "    temporal_context: Optional[str] = None\n",
    "    date_mentioned: Optional[datetime] = None\n",
    "    source_context: Optional[str] = None\n",
    "\n",
    "class Newsletter(BaseModel):\n",
    "    \"\"\"Represents a newsletter to be processed.\"\"\"\n",
    "    html_content: str\n",
    "    subject: str\n",
    "    sender: str\n",
    "    received_date: Optional[datetime] = None\n",
    "    newsletter_id: Optional[str] = None\n",
    "\n",
    "class ExtractionState(TypedDict):\n",
    "    \"\"\"State for the LangGraph workflow.\"\"\"\n",
    "    # Input\n",
    "    newsletter: Newsletter\n",
    "    \n",
    "    # Processing stages\n",
    "    cleaned_text: str\n",
    "    extracted_entities: List[Entity]\n",
    "    resolved_entities: List[Entity]\n",
    "    extracted_facts: List[Fact]\n",
    "    \n",
    "    # Results\n",
    "    neo4j_updates: Dict[str, Any]\n",
    "    processing_metrics: Dict[str, Any]\n",
    "    text_summary: str\n",
    "    errors: List[str]\n",
    "    \n",
    "    # Metadata\n",
    "    processing_start_time: datetime\n",
    "    current_step: str\n",
    "\n",
    "print(\"üìä Data models defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sample Newsletter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample newsletter HTML for testing\n",
    "sample_newsletter_html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>AI Weekly Newsletter #245</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>AI Weekly Newsletter #245</h1>\n",
    "    <p>Welcome to this week's AI updates!</p>\n",
    "    \n",
    "    <h2>üöÄ Major Announcements</h2>\n",
    "    <p><strong>OpenAI</strong> announced the release of <strong>GPT-5</strong> at their developer conference in <strong>San Francisco</strong>. \n",
    "    CEO <strong>Sam Altman</strong> presented the new capabilities during the <strong>OpenAI DevDay 2024</strong> event.</p>\n",
    "    \n",
    "    <h2>üè¢ Company Updates</h2>\n",
    "    <p><strong>Microsoft</strong> expanded their <strong>Azure AI</strong> services with new enterprise features. \n",
    "    The announcement was made by <strong>Satya Nadella</strong> during the <strong>Microsoft Build 2024</strong> conference.</p>\n",
    "    \n",
    "    <h2>üéØ Industry News</h2>\n",
    "    <p><strong>Google</strong> launched their new <strong>Gemini Pro</strong> model, focusing on <strong>AI Safety</strong> and \n",
    "    <strong>Responsible AI</strong> development. The launch event was held at <strong>Google I/O 2024</strong> in <strong>Mountain View</strong>.</p>\n",
    "    \n",
    "    <h2>üéì Educational Content</h2>\n",
    "    <p><strong>Stanford University</strong> announced a new <strong>AI Safety</strong> course taught by renowned professor \n",
    "    <strong>Fei-Fei Li</strong>. The course will cover <strong>Machine Learning</strong> ethics and <strong>AI Alignment</strong>.</p>\n",
    "    \n",
    "    <h2>üí° Research Highlights</h",
    "    <p>New research on <strong>Quantum Computing</strong> applications in <strong>AI</strong> was published by researchers at \n",
    "    <strong>MIT</strong> and <strong>IBM Research</strong>. The paper explores <strong>Quantum Machine Learning</strong> algorithms.</p>\n",
    "    \n",
    "    <p>Thanks for reading! See you next week.</p>\n",
    "    <p>Best regards,<br>The AI Weekly Team</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Create sample newsletter object\n",
    "sample_newsletter = Newsletter(\n",
    "    html_content=sample_newsletter_html,\n",
    "    subject=\"AI Weekly Newsletter #245\",\n",
    "    sender=\"ai-weekly@example.com\",\n",
    "    received_date=datetime.now(timezone.utc),\n",
    "    newsletter_id=\"ai-weekly-245\"\n",
    ")\n",
    "\n",
    "print(\"üìß Sample newsletter created:\")\n",
    "print(f\"  Subject: {sample_newsletter.subject}\")\n",
    "print(f\"  Sender: {sample_newsletter.sender}\")\n",
    "print(f\"  Content length: {len(sample_newsletter.html_content)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. HTML Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html_content(html_content: str) -> str:\n",
    "    \"\"\"Clean and extract text from HTML content.\"\"\"\n",
    "    try:\n",
    "        # Parse HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Convert to text using html2text for better formatting\n",
    "        h = html2text.HTML2Text()\n",
    "        h.ignore_links = False\n",
    "        h.ignore_images = True\n",
    "        h.body_width = 0  # Don't wrap lines\n",
    "        \n",
    "        # Get text content\n",
    "        text_content = h.handle(str(soup))\n",
    "        \n",
    "        # Clean up extra whitespace\n",
    "        cleaned_text = ' '.join(text_content.split())\n",
    "        \n",
    "        return cleaned_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cleaning HTML: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_sections(html_content: str) -> Dict[str, str]:\n",
    "    \"\"\"Extract different sections of the newsletter.\"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        sections = {\n",
    "            'title': '',\n",
    "            'headers': [],\n",
    "            'paragraphs': [],\n",
    "            'links': []\n",
    "        }\n",
    "        \n",
    "        # Extract title\n",
    "        title_tag = soup.find('title') or soup.find('h1')\n",
    "        if title_tag:\n",
    "            sections['title'] = title_tag.get_text().strip()\n",
    "        \n",
    "        # Extract headers\n",
    "        for header in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "            sections['headers'].append(header.get_text().strip())\n",
    "        \n",
    "        # Extract paragraphs\n",
    "        for para in soup.find_all('p'):\n",
    "            text = para.get_text().strip()\n",
    "            if text:\n",
    "                sections['paragraphs'].append(text)\n",
    "        \n",
    "        # Extract links\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            sections['links'].append({\n",
    "                'text': link.get_text().strip(),\n",
    "                'url': link['href']\n",
    "            })\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting sections: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Test HTML processing\n",
    "cleaned_text = clean_html_content(sample_newsletter.html_content)\n",
    "sections = extract_text_sections(sample_newsletter.html_content)\n",
    "\n",
    "print(\"üßπ HTML Processing Results:\")\n",
    "print(f\"  Cleaned text length: {len(cleaned_text)} characters\")\n",
    "print(f\"  Title: {sections.get('title', 'N/A')}\")\n",
    "print(f\"  Headers found: {len(sections.get('headers', []))}\")\n",
    "print(f\"  Paragraphs found: {len(sections.get('paragraphs', []))}\")\n",
    "print(f\"  Links found: {len(sections.get('links', []))}\")\n",
    "\n",
    "# Show first 200 characters of cleaned text\n",
    "print(f\"\\nFirst 200 characters of cleaned text:\")\n",
    "print(f\"'{cleaned_text[:200]}...'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LLM Integration for Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "if config.OPENAI_API_KEY:\n",
    "    openai_client = OpenAI(api_key=config.OPENAI_API_KEY)\n",
    "    llm = ChatOpenAI(\n",
    "        model=config.LLM_MODEL,\n",
    "        temperature=config.LLM_TEMPERATURE,\n",
    "        max_tokens=config.LLM_MAX_TOKENS,\n",
    "        openai_api_key=config.OPENAI_API_KEY\n",
    "    )\n",
    "    print(\"‚úÖ OpenAI client initialized\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è OpenAI API key not found. Please set OPENAI_API_KEY in your .env file\")\n",
    "    llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity extraction prompt template\n",
    "ENTITY_EXTRACTION_PROMPT = \"\"\"\n",
    "You are an expert at extracting structured information from newsletter content. \n",
    "Extract entities from the following newsletter text and classify them into these categories:\n",
    "\n",
    "**Entity Types:**\n",
    "- **Organization**: Companies, institutions, government bodies\n",
    "- **Person**: Individuals mentioned in content\n",
    "- **Product**: Software, hardware, services, models\n",
    "- **Event**: Conferences, announcements, launches\n",
    "- **Location**: Geographic locations (cities, countries, regions)\n",
    "- **Topic**: Subject areas, technologies, fields of study\n",
    "\n",
    "**Instructions:**\n",
    "1. Extract entities with high confidence (>0.7)\n",
    "2. Provide alternative names/aliases if mentioned\n",
    "3. Include context where the entity was mentioned\n",
    "4. Rate confidence from 0.0 to 1.0\n",
    "5. Return results as valid JSON\n",
    "\n",
    "**Newsletter Content:**\n",
    "{content}\n",
    "\n",
    "**Required JSON Format:**\n",
    "```json\n",
    "{\n",
    "  \"entities\": [\n",
    "    {\n",
    "      \"name\": \"Entity Name\",\n",
    "      \"type\": \"Organization|Person|Product|Event|Location|Topic\",\n",
    "      \"aliases\": [\"Alternative Name 1\", \"Alternative Name 2\"],\n",
    "      \"confidence\": 0.95,\n",
    "      \"context\": \"The sentence or phrase where this entity was mentioned\",\n",
    "      \"properties\": {\n",
    "        \"additional_info\": \"any relevant details\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "Return only valid JSON, no additional text.\n",
    "\"\"\"\n",
    "\n",
    "def extract_entities_with_llm(content: str) -> List[Entity]:\n",
    "    \"\"\"Extract entities from content using LLM.\"\"\"\n",
    "    if not llm:\n",
    "        print(\"‚ö†Ô∏è LLM not initialized\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Create prompt\n",
    "        prompt = ENTITY_EXTRACTION_PROMPT.format(content=content)\n",
    "        \n",
    "        # Get LLM response\n",
    "        response = llm.invoke(prompt)\n",
    "        \n",
    "        # Parse JSON response\n",
    "        import json\n",
    "        result = json.loads(response.content)\n",
    "        \n",
    "        # Convert to Entity objects\n",
    "        entities = []\n",
    "        for entity_data in result.get('entities', []):\n",
    "            entity = Entity(\n",
    "                name=entity_data['name'],\n",
    "                type=entity_data['type'],\n",
    "                aliases=entity_data.get('aliases', []),\n",
    "                confidence=entity_data['confidence'],\n",
    "                context=entity_data.get('context'),\n",
    "                properties=entity_data.get('properties', {})\n",
    "            )\n",
    "            entities.append(entity)\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting entities: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test entity extraction (only if LLM is available)\n",
    "if llm:\n",
    "    print(\"üîç Testing entity extraction...\")\n",
    "    extracted_entities = extract_entities_with_llm(cleaned_text[:1000])  # Test with first 1000 chars\n",
    "    \n",
    "    if extracted_entities:\n",
    "        print(f\"‚úÖ Extracted {len(extracted_entities)} entities:\")\n",
    "        for i, entity in enumerate(extracted_entities[:5]):  # Show first 5\n",
    "            print(f\"  {i+1}. {entity.name} ({entity.type}) - Confidence: {entity.confidence}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No entities extracted\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping entity extraction test - LLM not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Neo4j Graph Database Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neo4j Connection Manager\n",
    "class Neo4jConnection:\n",
    "    def __init__(self, uri: str, user: str, password: str):\n",
    "        self.uri = uri\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.driver = None\n",
    "        \n",
    "    def connect(self):\n",
    "        \"\"\"Establish connection to Neo4j.\"\"\"\n",
    "        try:\n",
    "            self.driver = GraphDatabase.driver(\n",
    "                self.uri, \n",
    "                auth=(self.user, self.password)\n",
    "            )\n",
    "            # Test connection\n",
    "            with self.driver.session() as session:\n",
    "                result = session.run(\"RETURN 1 as test\")\n",
    "                result.single()\n",
    "            print(\"‚úÖ Neo4j connection established\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Neo4j connection failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close Neo4j connection.\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.close()\n",
    "            print(\"üîå Neo4j connection closed\")\n",
    "    \n",
    "    def execute_query(self, query: str, parameters: dict = None):\n",
    "        \"\"\"Execute a Cypher query.\"\"\"\n",
    "        if not self.driver:\n",
    "            print(\"‚ùå No Neo4j connection\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            with self.driver.session() as session:\n",
    "                result = session.run(query, parameters or {})\n",
    "                return result.data()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Query execution failed: {e}\")\n",
    "            return None\n",
    "\n",
    "# Initialize Neo4j connection\n",
    "neo4j_conn = Neo4jConnection(\n",
    "    uri=config.NEO4J_URI,\n",
    "    user=config.NEO4J_USER,\n",
    "    password=config.NEO4J_PASSWORD\n",
    ")\n",
    "\n",
    "# Test connection\n",
    "connection_success = neo4j_conn.connect()\n",
    "if not connection_success:\n",
    "    print(\"‚ö†Ô∏è Neo4j connection failed. Please check your configuration.\")\n",
    "    print(\"   Make sure Neo4j is running and credentials are correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph database schema setup\n",
    "def setup_graph_constraints_and_indexes():\n",
    "    \"\"\"Create constraints and indexes for optimal graph performance.\"\"\"\n",
    "    if not neo4j_conn.driver:\n",
    "        print(\"‚ùå No Neo4j connection available\")\n",
    "        return\n",
    "    \n",
    "    constraints_and_indexes = [\n",
    "        # Unique constraints\n",
    "        \"CREATE CONSTRAINT unique_org_name IF NOT EXISTS FOR (o:Organization) REQUIRE o.name IS UNIQUE\",\n",
    "        \"CREATE CONSTRAINT unique_person_name IF NOT EXISTS FOR (p:Person) REQUIRE p.name IS UNIQUE\",\n",
    "        \"CREATE CONSTRAINT unique_product_name IF NOT EXISTS FOR (pr:Product) REQUIRE pr.name IS UNIQUE\",\n",
    "        \"CREATE CONSTRAINT unique_event_name IF NOT EXISTS FOR (e:Event) REQUIRE e.name IS UNIQUE\",\n",
    "        \"CREATE CONSTRAINT unique_location_name IF NOT EXISTS FOR (l:Location) REQUIRE l.name IS UNIQUE\",\n",
    "        \"CREATE CONSTRAINT unique_topic_name IF NOT EXISTS FOR (t:Topic) REQUIRE t.name IS UNIQUE\",\n",
    "        \"CREATE CONSTRAINT unique_newsletter_id IF NOT EXISTS FOR (n:Newsletter) REQUIRE n.id IS UNIQUE\",\n",
    "        \n",
    "        # Performance indexes\n",
    "        \"CREATE INDEX newsletter_date_idx IF NOT EXISTS FOR (n:Newsletter) ON (n.received_date)\",\n",
    "        \"CREATE INDEX entity_confidence_idx IF NOT EXISTS FOR (e:Organization) ON (e.confidence)\",\n",
    "        \"CREATE INDEX entity_last_seen_idx IF NOT EXISTS FOR (e:Organization) ON (e.last_seen)\",\n",
    "    ]\n",
    "    \n",
    "    print(\"üîß Setting up graph constraints and indexes...\")\n",
    "    \n",
    "    for constraint in constraints_and_indexes:\n",
    "        try:\n",
    "            neo4j_conn.execute_query(constraint)\n",
    "            print(f\"  ‚úÖ {constraint.split()[1]} {constraint.split()[2]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è {constraint.split()[1]} {constraint.split()[2]}: {e}\")\n",
    "    \n",
    "    print(\"üìä Graph schema setup complete!\")\n",
    "\n",
    "# Set up schema (only if connected)\n",
    "if connection_success:\n",
    "    setup_graph_constraints_and_indexes()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping schema setup - no Neo4j connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Graph Operations Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph operations for entity management\n",
    "class GraphOperations:\n",
    "    def __init__(self, neo4j_connection):\n",
    "        self.neo4j_conn = neo4j_connection\n",
    "    \n",
    "    def create_or_update_entity(self, entity: Entity) -> dict:\n",
    "        \"\"\"Create or update an entity node in the graph.\"\"\"\n",
    "        query = f\"\"\"\n",
    "        MERGE (e:{entity.type} {{name: $name}})\n",
    "        ON CREATE SET \n",
    "            e.created_at = datetime(),\n",
    "            e.confidence = $confidence,\n",
    "            e.aliases = $aliases,\n",
    "            e.mention_count = 1,\n",
    "            e.properties = $properties\n",
    "        ON MATCH SET\n",
    "            e.last_seen = datetime(),\n",
    "            e.mention_count = e.mention_count + 1,\n",
    "            e.confidence = CASE \n",
    "                WHEN $confidence > e.confidence THEN $confidence \n",
    "                ELSE e.confidence \n",
    "            END\n",
    "        RETURN e, \n",
    "               CASE WHEN e.created_at = e.last_seen THEN 'created' ELSE 'updated' END as operation\n",
    "        \"\"\"\n",
    "        \n",
    "        parameters = {\n",
    "            'name': entity.name,\n",
    "            'confidence': entity.confidence,\n",
    "            'aliases': entity.aliases,\n",
    "            'properties': entity.properties\n",
    "        }\n",
    "        \n",
    "        result = self.neo4j_conn.execute_query(query, parameters)\n",
    "        return result[0] if result else None\n",
    "    \n",
    "    def create_newsletter_node(self, newsletter: Newsletter) -> dict:\n",
    "        \"\"\"Create a newsletter node in the graph.\"\"\"\n",
    "        query = \"\"\"\n",
    "        CREATE (n:Newsletter {\n",
    "            id: $newsletter_id,\n",
    "            subject: $subject,\n",
    "            sender: $sender,\n",
    "            received_date: $received_date,\n",
    "            created_at: datetime(),\n",
    "            content_length: $content_length\n",
    "        })\n",
    "        RETURN n\n",
    "        \"\"\"\n",
    "        \n",
    "        parameters = {\n",
    "            'newsletter_id': newsletter.newsletter_id,\n",
    "            'subject': newsletter.subject,\n",
    "            'sender': newsletter.sender,\n",
    "            'received_date': newsletter.received_date,\n",
    "            'content_length': len(newsletter.html_content)\n",
    "        }\n",
    "        \n",
    "        result = self.neo4j_conn.execute_query(query, parameters)\n",
    "        return result[0] if result else None\n",
    "    \n",
    "    def link_entity_to_newsletter(self, entity_name: str, entity_type: str, newsletter_id: str, context: str = None) -> dict:\n",
    "        \"\"\"Create a MENTIONED_IN relationship between entity and newsletter.\"\"\"\n",
    "        query = f\"\"\"\n",
    "        MATCH (e:{entity_type} {{name: $entity_name}})\n",
    "        MATCH (n:Newsletter {{id: $newsletter_id}})\n",
    "        CREATE (e)-[r:MENTIONED_IN {{\n",
    "            date: datetime(),\n",
    "            context: $context\n",
    "        }}]->(n)\n",
    "        RETURN r\n",
    "        \"\"\"\n",
    "        \n",
    "        parameters = {\n",
    "            'entity_name': entity_name,\n",
    "            'newsletter_id': newsletter_id,\n",
    "            'context': context\n",
    "        }\n",
    "        \n",
    "        result = self.neo4j_conn.execute_query(query, parameters)\n",
    "        return result[0] if result else None\n",
    "    \n",
    "    def find_similar_entities(self, entity_name: str, entity_type: str, similarity_threshold: float = 0.8) -> List[dict]:\n",
    "        \"\"\"Find entities with similar names for resolution.\"\"\"\n",
    "        query = f\"\"\"\n",
    "        MATCH (e:{entity_type})\n",
    "        WHERE e.name CONTAINS $search_term \n",
    "           OR ANY(alias IN e.aliases WHERE alias CONTAINS $search_term)\n",
    "           OR $search_term CONTAINS e.name\n",
    "        RETURN e, \n",
    "               e.mention_count as popularity,\n",
    "               e.confidence as confidence\n",
    "        ORDER BY popularity DESC, confidence DESC\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "        \n",
    "        parameters = {'search_term': entity_name}\n",
    "        result = self.neo4j_conn.execute_query(query, parameters)\n",
    "        return result or []\n",
    "    \n",
    "    def get_graph_stats(self) -> dict:\n",
    "        \"\"\"Get basic statistics about the graph.\"\"\"\n",
    "        stats_query = \"\"\"\n",
    "        CALL {\n",
    "            MATCH (o:Organization) RETURN count(o) as organizations\n",
    "        }\n",
    "        CALL {\n",
    "            MATCH (p:Person) RETURN count(p) as people\n",
    "        }\n",
    "        CALL {\n",
    "            MATCH (pr:Product) RETURN count(pr) as products\n",
    "        }\n",
    "        CALL {\n",
    "            MATCH (e:Event) RETURN count(e) as events\n",
    "        }\n",
    "        CALL {\n",
    "            MATCH (l:Location) RETURN count(l) as locations\n",
    "        }\n",
    "        CALL {\n",
    "            MATCH (t:Topic) RETURN count(t) as topics\n",
    "        }\n",
    "        CALL {\n",
    "            MATCH (n:Newsletter) RETURN count(n) as newsletters\n",
    "        }\n",
    "        CALL {\n",
    "            MATCH ()-[r]->() RETURN count(r) as relationships\n",
    "        }\n",
    "        RETURN organizations, people, products, events, locations, topics, newsletters, relationships\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self.neo4j_conn.execute_query(stats_query)\n",
    "        return result[0] if result else {}\n",
    "\n",
    "# Initialize graph operations\n",
    "if connection_success:\n",
    "    graph_ops = GraphOperations(neo4j_conn)\n",
    "    \n",
    "    # Get initial graph statistics\n",
    "    stats = graph_ops.get_graph_stats()\n",
    "    print(\"üìä Current graph statistics:\")\n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key.capitalize()}: {value}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Graph operations not available - no Neo4j connection\")\n",
    "    graph_ops = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Complete Processing Pipeline Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete processing pipeline function\n",
    "def process_newsletter_pipeline(newsletter: Newsletter) -> dict:\n",
    "    \"\"\"Process a newsletter through the complete pipeline.\"\"\"\n",
    "    start_time = datetime.now()\n",
    "    results = {\n",
    "        'status': 'success',\n",
    "        'processing_time': 0,\n",
    "        'entities_extracted': 0,\n",
    "        'entities_new': 0,\n",
    "        'entities_updated': 0,\n",
    "        'newsletter_node_id': newsletter.newsletter_id,\n",
    "        'summary': {},\n",
    "        'text_summary': '',\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"üöÄ Starting newsletter processing: {newsletter.subject}\")\n",
    "        \n",
    "        # Step 1: Clean HTML content\n",
    "        print(\"  1Ô∏è‚É£ Cleaning HTML content...\")\n",
    "        cleaned_text = clean_html_content(newsletter.html_content)\n",
    "        if not cleaned_text:\n",
    "            results['errors'].append(\"Failed to clean HTML content\")\n",
    "            results['status'] = 'error'\n",
    "            return results\n",
    "        \n",
    "        # Step 2: Extract entities with LLM\n",
    "        print(\"  2Ô∏è‚É£ Extracting entities with LLM...\")\n",
    "        if llm:\n",
    "            entities = extract_entities_with_llm(cleaned_text)\n",
    "            results['entities_extracted'] = len(entities)\n",
    "            print(f\"    ‚úÖ Extracted {len(entities)} entities\")\n",
    "        else:\n",
    "            entities = []\n",
    "            results['errors'].append(\"LLM not available for entity extraction\")\n",
    "        \n",
    "        # Step 3: Create newsletter node in graph\n",
    "        print(\"  3Ô∏è‚É£ Creating newsletter node...\")\n",
    "        if graph_ops:\n",
    "            newsletter_node = graph_ops.create_newsletter_node(newsletter)\n",
    "            if newsletter_node:\n",
    "                print(\"    ‚úÖ Newsletter node created\")\n",
    "            else:\n",
    "                results['errors'].append(\"Failed to create newsletter node\")\n",
    "        else:\n",
    "            results['errors'].append(\"Graph operations not available\")\n",
    "        \n",
    "        # Step 4: Process entities and create/update nodes\n",
    "        print(\"  4Ô∏è‚É£ Processing entities in graph...\")\n",
    "        entity_summary = {'Organization': 0, 'Person': 0, 'Product': 0, 'Event': 0, 'Location': 0, 'Topic': 0}\n",
    "        new_entities = 0\n",
    "        updated_entities = 0\n",
    "        \n",
    "        if graph_ops and entities:\n",
    "            for entity in entities:\n",
    "                try:\n",
    "                    # Create or update entity\n",
    "                    result = graph_ops.create_or_update_entity(entity)\n",
    "                    if result:\n",
    "                        if result.get('operation') == 'created':\n",
    "                            new_entities += 1\n",
    "                        else:\n",
    "                            updated_entities += 1\n",
    "                        \n",
    "                        # Link to newsletter\n",
    "                        graph_ops.link_entity_to_newsletter(\n",
    "                            entity.name, \n",
    "                            entity.type, \n",
    "                            newsletter.newsletter_id, \n",
    "                            entity.context\n",
    "                        )\n",
    "                        \n",
    "                        # Update summary\n",
    "                        entity_summary[entity.type] += 1\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    results['errors'].append(f\"Error processing entity {entity.name}: {e}\")\n",
    "            \n",
    "            print(f\"    ‚úÖ Created {new_entities} new entities, updated {updated_entities} existing entities\")\n",
    "        \n",
    "        # Step 5: Generate summary\n",
    "        print(\"  5Ô∏è‚É£ Generating summary...\")\n",
    "        results['entities_new'] = new_entities\n",
    "        results['entities_updated'] = updated_entities\n",
    "        results['summary'] = entity_summary\n",
    "        \n",
    "        # Create text summary\n",
    "        entity_mentions = []\n",
    "        for entity_type, count in entity_summary.items():\n",
    "            if count > 0:\n",
    "                entity_mentions.append(f\"{count} {entity_type.lower()}{'s' if count > 1 else ''}\")\n",
    "        \n",
    "        processing_time = (datetime.now() - start_time).total_seconds()\n",
    "        results['processing_time'] = processing_time\n",
    "        \n",
    "        results['text_summary'] = f\"\"\"Processed newsletter '{newsletter.subject}' from {newsletter.sender}. \n",
    "Extracted {results['entities_extracted']} entities ({', '.join(entity_mentions) if entity_mentions else 'none'}). \n",
    "Created {new_entities} new entities, updated {updated_entities} existing entities. \n",
    "Processing completed in {processing_time:.2f} seconds.\"\"\"\n",
    "        \n",
    "        print(f\"‚úÖ Processing completed in {processing_time:.2f} seconds\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        results['status'] = 'error'\n",
    "        results['errors'].append(f\"Pipeline error: {e}\")\n",
    "        results['processing_time'] = (datetime.now() - start_time).total_seconds()\n",
    "        print(f\"‚ùå Processing failed: {e}\")\n",
    "        return results\n",
    "\n",
    "# Test the complete pipeline\n",
    "print(\"üîÑ Testing complete newsletter processing pipeline...\")\n",
    "pipeline_results = process_newsletter_pipeline(sample_newsletter)\n",
    "\n",
    "print(\"\\nüìä Pipeline Results:\")\n",
    "print(f\"  Status: {pipeline_results['status']}\")\n",
    "print(f\"  Processing time: {pipeline_results['processing_time']:.2f} seconds\")\n",
    "print(f\"  Entities extracted: {pipeline_results['entities_extracted']}\")\n",
    "print(f\"  New entities: {pipeline_results['entities_new']}\")\n",
    "print(f\"  Updated entities: {pipeline_results['entities_updated']}\")\n",
    "print(f\"  Entity summary: {pipeline_results['summary']}\")\n",
    "\n",
    "if pipeline_results['errors']:\n",
    "    print(f\"  Errors: {pipeline_results['errors']}\")\n",
    "\n",
    "print(f\"\\nüìù Text Summary:\")\n",
    "print(pipeline_results['text_summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze and visualize results\n",
    "if graph_ops and pipeline_results['status'] == 'success':\n",
    "    # Get updated graph statistics\n",
    "    updated_stats = graph_ops.get_graph_stats()\n",
    "    \n",
    "    print(\"üìä Updated graph statistics:\")\n",
    "    for key, value in updated_stats.items():\n",
    "        print(f\"  {key.capitalize()}: {value}\")\n",
    "    \n",
    "    # Create visualization of entity types\n",
    "    entity_data = pipeline_results['summary']\n",
    "    entity_types = list(entity_data.keys())\n",
    "    entity_counts = list(entity_data.values())\n",
    "    \n",
    "    if sum(entity_counts) > 0:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Bar chart of entity types\n",
    "        bars = ax1.bar(entity_types, entity_counts, color=sns.color_palette(\"husl\", len(entity_types)))\n",
    "        ax1.set_title('Extracted Entities by Type')\n",
    "        ax1.set_xlabel('Entity Type')\n",
    "        ax1.set_ylabel('Count')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, count in zip(bars, entity_counts):\n",
    "            if count > 0:\n",
    "                ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                        str(count), ha='center', va='bottom')\n",
    "        \n",
    "        # Pie chart of entity distribution\n",
    "        non_zero_types = [t for t, c in zip(entity_types, entity_counts) if c > 0]\n",
    "        non_zero_counts = [c for c in entity_counts if c > 0]\n",
    "        \n",
    "        if non_zero_counts:\n",
    "            ax2.pie(non_zero_counts, labels=non_zero_types, autopct='%1.1f%%', startangle=90)\n",
    "            ax2.set_title('Entity Distribution')\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, 'No entities extracted', ha='center', va='center', transform=ax2.transAxes)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"üìä No entities to visualize\")\n",
    "else:\n",
    "    print(\"üìä Visualization skipped - no successful results or graph connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps and Production Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development Summary\n",
    "\n",
    "This notebook has successfully implemented and tested the core components of the newsletter processing pipeline:\n",
    "\n",
    "‚úÖ **Completed Components:**\n",
    "- Configuration management with environment variables\n",
    "- HTML processing and text extraction\n",
    "- LLM integration for entity extraction\n",
    "- Neo4j graph database connection and operations\n",
    "- Entity creation and relationship management\n",
    "- Complete processing pipeline\n",
    "- Results analysis and visualization\n",
    "\n",
    "### Next Development Steps:\n",
    "\n",
    "1. **Fact Extraction**: Implement relationship extraction between entities\n",
    "2. **Temporal Processing**: Add time-based relationship handling\n",
    "3. **Entity Resolution**: Improve fuzzy matching and disambiguation\n",
    "4. **Batch Processing**: Handle multiple newsletters efficiently\n",
    "5. **Error Handling**: Add comprehensive error recovery\n",
    "6. **Performance Optimization**: Optimize LLM calls and graph operations\n",
    "\n",
    "### Production Migration:\n",
    "\n",
    "1. **Extract Code to Modules**: Move notebook functions to `src/` directory\n",
    "2. **FastAPI Integration**: Create REST endpoint using developed logic\n",
    "3. **Testing**: Implement comprehensive test suite\n",
    "4. **Deployment**: Configure for Cloud Run deployment\n",
    "5. **Monitoring**: Add logging and metrics collection\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "- The pipeline successfully processes newsletter content end-to-end\n",
    "- Entity extraction quality depends on LLM prompt engineering\n",
    "- Graph operations are efficient for entity management\n",
    "- Real-time processing is feasible for typical newsletter volumes\n",
    "- Error handling is critical for production reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up connections\n",
    "if neo4j_conn:\n",
    "    neo4j_conn.close()\n",
    "\n",
    "print(\"üßπ Cleanup completed\")\n",
    "print(\"\\nüéâ Newsletter processing pipeline development complete!\")\n",
    "print(\"\\nüìã Ready for production migration:\")\n",
    "print(\"  1. Extract functions to src/ modules\")\n",
    "print(\"  2. Create FastAPI endpoints\")\n",
    "print(\"  3. Add comprehensive testing\")\n",
    "print(\"  4. Configure deployment\")\n",
    "print(\"  5. Set up monitoring\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}